# Technical Architecture Review: SyncWell

## 1. Summary of Findings

### Overall Maturity
The collection of documents represents a **mature and comprehensive** architectural plan. The author(s) have demonstrated a deep understanding of cloud-native patterns, serverless architectures, and the specific challenges of building a data synchronization service. The use of the C4 model, detailed diagrams, and explicit articulation of non-functional requirements is commendable. The proactive consideration of security, scalability, and developer experience places this document in the top tier of technical specifications.

However, the architecture's greatest strength—its ambition and completeness—is also the source of its primary weaknesses. The plan is **highly complex**, spanning numerous documents and advanced AWS services. This complexity has introduced **significant inconsistencies, ambiguities, and critical risks** that must be addressed before implementation.

### Critical Findings
Three findings stand out as requiring immediate, project-level attention:

1.  **Major Contradiction in Core Deployment Strategy (Finding #12):** The `06-technical-architecture.md` document contains a direct contradiction regarding the MVP's database strategy. One section claims the `SyncWellMetadata` table will be a single-region DynamoDB table to reduce MVP complexity, while the High Availability section in the *same document* mandates a multi-region deployment using DynamoDB Global Tables. This is the most critical inconsistency, as it fundamentally alters the entire architecture, cost model, and operational plan.

2.  **Catastrophic Cost & Feasibility Projections (Finding #68):** The projection of requiring **45,000 concurrent Lambda executions** to meet the 3,000 RPS target is a project-threatening risk. The document acknowledges this but defers the detailed cost modeling that should, in fact, be a prerequisite for accepting the architecture. The financial and technical feasibility of this projection is questionable and could render the entire approach non-viable.

3.  **Unsafe Manual Operational Procedures (Finding #72, #94):** The proposed manual runbooks for "hot user" migration and manual account recovery are exceptionally risky. Performing multi-step, destructive operations manually on a production database at scale is a recipe for catastrophic human error. These processes are not scalable and introduce an unacceptable level of risk for a system designed for 1M DAU.

---

## 2. Logical Flaws, Bugs, and Inconsistencies

This section details direct contradictions, logical errors, and inaccuracies across the documents.

| ID | Description | Impact/Severity | Recommendation |
| :--- | :--- | :--- | :--- |
| **1** | **Contradictory ElastiCache Recovery Strategy:** `18-backup-recovery.md` states the RTO for a cache cluster failure is < 60 minutes via a *manual* promotion. However, `06-technical-architecture.md` states the ElastiCache cluster *must* be deployed in a **Multi-AZ configuration**, which provides *automatic* failover with an RTO of minutes, not hours. | **High** | The recovery plan and the technical architecture must be aligned. Clarify if the cluster is Multi-AZ (with automatic failover) or requires a manual promotion. The Multi-AZ approach is strongly recommended. |
| **2** | **Flawed Idempotency Logic in Diagram:** The idempotency sequence diagram in `06-technical-architecture.md` shows the worker setting a key to `INPROGRESS` and then checking for `nil` to detect a duplicate. This logic is flawed. It doesn't account for a key that already exists with a `COMPLETED` state, which would incorrectly be treated as a duplicate in progress. | **High** | The diagram's logic must be corrected to reflect a more robust check: 1. Check if key exists. 2. If yes, check its state (`INPROGRESS` vs. `COMPLETED`). 3. If no, create the lock. The text describes a better flow than the diagram; the diagram should be updated to match the more nuanced text. |
| **3** | **Inconsistent Idempotency Implementation Details:** The text in `06-technical-architecture.md` describes a simple idempotency check in the worker. The sequence diagram, however, depicts a much more complex two-phase lock (`INPROGRESS` state with a short TTL, then a `COMPLETED` state). These are two different implementations. | **Medium** | The documentation must be consistent. Decide on one implementation (the two-phase lock in the diagram is more robust and generally recommended) and update the text to match the diagram's logic. |
| **4** | **Contradiction on Backend Data Persistence:** `06-technical-architecture.md` states "The backend does not **persist** any raw user health data". However, `19-security-privacy.md` details the `SyncWellBreakGlassIndex`, which persists `userId` (PII) and `correlationId` for 72 hours. The "never persist" claim is overly broad and factually incorrect. | **Medium** | Refine the "no persistence" statement to be more precise. For example: "The backend does not persist any user health data, and any temporary diagnostic metadata containing user identifiers is stored in a secure, audited, time-limited index." |
| **5** | **Contradictory Statements on API Gateway Responsibilities:** `06-technical-architecture.md` initially states "The API Layer (API Gateway) is responsible for request validation, authorization, and routing." Later, section `3a. Unified End-to-End Idempotency Strategy` clarifies that API Gateway "does not perform any idempotency checks itself." The initial statement is too broad. | **Low** | Make the initial, high-level description of API Gateway's responsibilities more precise to avoid implying it handles business-level validation like idempotency. |
| **6** | **Contradictory `DataProvider` SDK Packaging Strategy:** `07-apis-integration.md` states the SDK will be a versioned Maven package but then immediately suggests a simpler monorepo approach "could be considered" for the MVP. A technical specification must be decisive. | **Low** | Make a definitive decision for the MVP's `DataProvider` SDK strategy and remove the ambiguous language. The simpler in-repo module is likely sufficient for an MVP. |
| **7** | **Inconsistent API Endpoint for User Deletion:** In `06-technical-architecture.md`, the API contract for deleting a user is `DELETE /v1/user/me`. This is inconsistent with the endpoint for updating settings, which is `PUT /v1/users/me/settings` (plural `users`). | **Low** | Standardize the API endpoints for consistency. `DELETE /v1/users/me` is recommended to align with the other user-centric endpoints. |
| **8** | **Algorithmic Flaw in Historical Sync Chunking:** The chunking algorithm in `05-data-sync.md` relies on first querying the source provider for a total count of records in the date range. Not all third-party APIs support this kind of count query, and for those that do, it could be expensive or rate-limited. | **High** | The algorithm must be redesigned to not assume a "total count" feature exists. A better approach would be to iterate by time (e.g., month by month) and if a chunk contains too many records, dynamically split it into smaller time ranges. |
| **9** | **Logical Flaw in `ProviderTokens` KMP Implementation:** The `ProviderTokens` data class in `06-technical-architecture.md` uses `System.currentTimeMillis()`, which is a JVM-specific call. This contradicts the plan to potentially implement the security-critical `AuthorizerLambda` in Python or TypeScript, as this KMP code would not be portable to those runtimes. | **Medium** | This reveals an unstated assumption about KMP's usage. The `ProviderTokens` class, if shared, must use a KMP-compatible way of getting the current time (e.g., from a `Clock` interface). Alternatively, this logic should not be in a shared class if it's not truly cross-platform. |
| **10** | **Contradiction on Fitbit API Capability:** `07-apis-integration.md` clearly lists the Fitbit API as "Read-only for activity data" with a `Write Endpoint` of `N/A`. However, `05-data-sync.md` repeatedly uses "Sync Steps for User X from Fitbit to Google Fit" as a primary example, which implies data is read from Fitbit. This is confusing. A sync *from* Fitbit is a read operation, but the table makes it seem like the entire provider is read-only. | **Medium** | Clarify the `DataProvider` capabilities. The capability should be per-data-type. A provider might be read-only for workouts but read/write for another data type. The `capabilities` enum in the `DataProvider` interface should be more granular. The example is technically correct but confusing given the table's presentation. |
| **11** | **Logical Flaw in Conflict Detection Time Buffer:** The conflict detection algorithm in `05-data-sync.md` uses a time buffer (e.g., 5 minutes) around the source timestamp when fetching destination data. Why is this buffer applied to the *destination* query? The buffer is to account for clock skew between *source* and *destination*, so the query to the destination should simply be for the time range of the new source data. The buffer logic seems misplaced. | **Medium** | Re-evaluate the logic for the conflict detection query window. The query to the destination should likely be for the exact time range of the new source data. The conflict resolution engine can then apply a clock skew tolerance during the comparison. |
| **12** | **CRITICAL: Contradiction on Single-Region vs. Multi-Region MVP:** `06-technical-architecture.md` contains a severe contradiction. The "DynamoDB Data Modeling" section explicitly states: "For the MVP, the table will exist in a **single region**. It will not be configured as a Global Table." However, the "High Availability" section in the same document states the opposite: "The entire backend infrastructure will be deployed in at least **three geographically dispersed AWS regions**" and "User metadata... will be stored in a **DynamoDB Global Table**." | **Critical** | This is a fundamental contradiction that impacts the entire project's scope, cost, and complexity for the MVP. A definitive decision must be made immediately. Is the MVP single-region or multi-region? The entire set of documents must be updated to reflect this core decision. |
| **13** | **Inconsistent Error Handling for `AI-Powered Merge`:** `05-data-sync.md` states that if the AI service fails, the system will "log the error and automatically fall back to the default `Prioritize Source` strategy." However, `05-data-sync.md` also states that for partial failures, "the entire job will be considered failed. The worker will throw an error, allowing SQS to retry the job." An AI service failure is a partial failure. Will it cause a retry or a fallback? | **High** | Clarify the exact behavior. The fallback mechanism is a much better user experience than retrying the whole job. The specification should be updated to confirm that an AI service failure is a special case that triggers the fallback and does *not* cause the entire sync job to fail and be retried. |
| **14** | **Logical Flaw in DLQ Analyzer for "Bad Data":** The `DLQAnalyzer` in `17-error-handling.md` suggests that for "known 'Bad Data' formats," it will archive the message and trigger a low-priority ticket. This implies the bad data format is known. If it's known, why isn't the `DataProvider` coded to handle it gracefully before it causes a crash and a DLQ entry? | **Medium** | The primary mitigation for known bad data should be defensive coding in the `DataProvider`. The DLQ analyzer should be for *unknown* or *unrecoverable* errors. The strategy should be updated to reflect this. |
| **15** | **Contradiction on Rate Limiter Action:** `07-apis-integration.md` states that if a rate limit is hit, the `DataProvider` will "return the job to the SQS queue with a delay." `06-technical-architecture.md` states the worker will "Abort call and retry job later." These are subtly different. Returning to the queue with a delay is an SQS-specific feature (`ChangeMessageVisibility`). Aborting and retrying implies the Lambda fails and relies on the default redrive policy. | **Medium** | Specify the exact mechanism for rate limit backoff. Using SQS's `ChangeMessageVisibility` is more efficient and provides more control than a simple failure. The documentation should be standardized on this more precise implementation. |
| **16** | **Contradiction in Notification Trigger:** `29-notifications-alerts.md` states that the `N-01` "Sync Error" notification is triggered by the `DLQAnalyzer` service. This means a user is only notified after multiple retries have failed over a significant period. However, `07-apis-integration.md` states that for a `401 Unauthorized` error, the connection is *immediately* marked as `needs_reauth`. The user should be notified immediately, not after 5 retry attempts leading to a DLQ. | **High** | The notification trigger for permanent auth errors should be immediate. The worker itself should publish a `ReAuthenticationNeeded` event when it receives a 401/403 error, which can trigger a notification directly, rather than waiting for the DLQ process. |
| **17** | **Inconsistent Use of "Correlation ID":** `17-error-handling.md` states the `correlationId` is used to trace requests, and `userId` is never logged. `19-security-privacy.md` reinforces this, detailing a "break-glass" procedure to map a `userId` to a `correlationId`. However, the example log entry in `17-error-handling.md` includes `"jobId": "xyz-123"`. What is this `jobId`? Is it the idempotency key? Is it another traceable ID? The purpose and origin of `jobId` are unclear and it seems redundant with `correlationId`. | **Low** | Clarify what `jobId` represents in the structured logs. If it's the idempotency key, label it as such. If it's redundant, remove it to simplify the logging schema. |
| **18** | **Contradiction on Egress Firewall Implementation:** `06-technical-architecture.md` states that the egress firewall is `AWS Network Firewall`. `07-apis-integration.md` also states this. However, a simpler and more cost-effective way to achieve FQDN allow-listing for Lambda is to use a proxy, not the expensive and complex Network Firewall. Was a simpler solution considered? | **Medium** | The choice of AWS Network Firewall for this task seems like over-engineering. Justify why this was chosen over a simpler, cheaper alternative like a Squid proxy running on Fargate or EC2. The document should acknowledge the cost implications of this choice. |
| **19** | **Inconsistent Description of Historical Sync:** `06-technical-architecture.md` describes the historical sync as the "Cold Path." `05-data-sync.md` uses the same term. However, `07-apis-integration.md` introduces a `priority` field for the rate limiter, with values `"hot"` or `"cold"`. This implies a job is either hot or cold. But a historical sync is also composed of many individual worker jobs. The terminology is becoming overloaded and confusing. | **Low** | Refine the terminology. A "Historical Sync" is a long-running *workflow*. The individual tasks processed by that workflow have a *low priority* for rate-limiting purposes. Using "cold path" to describe both the workflow and the job priority is confusing. |
| **20** | **Logical Flaw in Chaos Engineering Plan:** The chaos engineering experiments in `06-technical-architecture.md` are excellent but contain a logical flaw. The "Secrets Manager Unavailability" test suggests workers with *cached credentials* would continue to function. What cached credentials? The architecture specifies that tokens are fetched from Secrets Manager for each job. There is no mention of an in-memory credential cache in the worker. | **Medium** | Correct the description of the chaos engineering test. If there is no credential cache, the test should verify that the worker fails gracefully and the job is retried, not that it continues to function. Alternatively, if a credential cache is desired, it must be formally added to the architecture. |

---

## 3. Clarity, Completeness, and Ambiguity

This section evaluates the quality of the documentation itself, identifying areas that are unclear, incomplete, or open to misinterpretation.

| ID | Description | Impact/Severity | Recommendation |
| :--- | :--- | :--- | :--- |
| **21** | **Ambiguous Definition of "Hybrid" for Google Fit:** `06-technical-architecture.md` defines the Google Fit integration as "Hybrid (Device & Cloud)" and states "The implementation will be device-first, using the cloud API as a fallback." This is ambiguous. What are the specific conditions that trigger a fallback to the cloud API? Is it an error? A specific data type? This is a critical implementation detail that is completely undefined. | **High** | Provide a precise definition of the "device-first, cloud-fallback" strategy for Google Fit. Detail the exact triggers and conditions under which the cloud API would be used instead of the on-device Health Connect SDK. |
| **22** | **Undefined Acronym "KMP":** `06-technical-architecture.md` uses the acronym "KMP" extensively (e.g., "Mobile Application w/ KMP Module") without ever defining it. A reader unfamiliar with the technology would not know this stands for Kotlin Multiplatform. | **Low** | Define the acronym "KMP" on its first use. E.g., "Kotlin Multiplatform (KMP)". |
| **23** | **Missing Information: `DataProvider` Role in Authentication:** The authentication flow in `07-apis-integration.md` is clear that the backend exchanges the auth code for tokens. However, the `DataProvider` interface in the same document has `authenticate` and `refreshAccessToken` methods. How does the `DataProvider` get the `authCode` to pass to its `authenticate` method? The role of the `DataProvider` in the initial, user-facing authentication flow is unclear. | **Medium** | Clarify the sequence of events. Does the API layer receive the auth code and then pass it to the correct `DataProvider` instance to handle the token exchange? The diagram and interface definition are currently disconnected. |
| **24** | **Vague Plan for "Last Writer Wins" Mitigation:** `06-technical-architecture.md` correctly identifies that DynamoDB's "last writer wins" is a high-risk strategy. The mitigation is "A more robust strategy... **must be implemented** for critical user configuration data before the feature set expands." This is too vague. Which feature set? What is the new strategy? | **High** | The document must specify the more robust conflict resolution strategy (e.g., version numbers with conditional writes) and define the exact trigger for its implementation. This is too critical to leave as a vague future task. |
| **25** | **Missing Diagram: Job Chunking Flow:** `16-performance-optimization.md` contains the text "[Placeholder - Diagram to be created]" for the Job Chunking Flow. | **Low** | Create and add the missing diagram to improve the document's clarity. |
| **26** | **Missing Diagram: Notification Data Flow:** `29-notifications-alerts.md` contains the text "[Placeholder - Diagram to be created]" for the Notification Data Flow. | **Low** | Create and add the missing diagram. |
| **27** | **Missing Diagram: Notification Settings Screen:** `29-notifications-alerts.md` contains the text "[Placeholder - Mockup to be created]" for the Notification Settings Screen. | **Low** | Create and add the missing mockup. |
| **28** | **Missing Diagram: Notification Banner Styles:** `29-notifications-alerts.md` contains the text "[Placeholder - Mockup to be created]" for the Notification Banner Styles. | **Low** | Create and add the missing mockup. |
| **29** | **Ambiguous Reference to a Removed Component:** `06-technical-architecture.md` in section `3a` mentions "This approach simplifies the architecture by removing the `RequestLambda`". For a reader who has not seen a previous version of the document, this is confusing. What was the `RequestLambda`? | **Low** | Remove the reference to the obsolete `RequestLambda` or add a brief footnote explaining what it was and why it was removed to avoid confusion. |
| **30** | **Unclear Diagram: `Level 2: Containers`:** The main architecture diagram in `06-technical-architecture.md` is extremely dense and difficult to read. Lines cross one another, and the data flow for components like the `AnonymizerProxy` is not immediately obvious. | **Medium** | Redraw the `Level 2: Containers` diagram to improve clarity. Consider breaking it into several smaller, more focused diagrams (e.g., one for the "Hot Path," one for the "Cold Path," one for the "Auth Flow"). |
| **31** | **Missing Information: Unit Conversion:** The canonical data models (e.g., `CanonicalWorkout`) specify units like `distanceMeters`. What if a provider returns data in miles or kilometers? The documents do not specify where and how unit conversions should happen. | **Medium** | Explicitly state that it is the responsibility of each `DataProvider` implementation to perform any necessary unit conversions to conform to the canonical model's standard units. |
| **32** | **Ambiguous Action for Jailbreak/Root Detection:** `19-security-privacy.md` states the app will perform "Jailbreak/Root Detection". This is notoriously unreliable. The document does not specify what action the app will take if a compromised OS is detected. Will it block the user? Log the event? Display a warning? | **Medium** | Specify the exact action to be taken upon detection of a potentially compromised OS. Acknowledge the risk of false positives and recommend a non-blocking action for the MVP (e.g., logging and warning). |
| **33** | **Missing Formal Definitions/Glossary:** The documents use terms like "Connection," "Sync Job," and "Provider" extensively. While their meaning can be inferred from context, a formal glossary of terms would improve clarity and ensure a shared understanding. | **Low** | Add a glossary section to the main `06-technical-architecture.md` document defining the key entities and concepts of the SyncWell ecosystem. |
| **34** | **Incomplete API Contract for Historical Sync:** The `POST /v1/sync-jobs` API in `06-technical-architecture.md` is used for both manual and historical syncs. The `dateRange` is optional. What happens if a user sets `mode` to `historical` but omits the `dateRange`? The API should return a `400 Bad Request`. | **Medium** | The API contract definition should explicitly state that `dateRange` is **required** when `mode` is `historical`, and this validation must be enforced. |
| **35** | **Missing Information: `ProviderTokens` Schema:** The `ProviderTokens` class is defined in `06-technical-architecture.md` but is missing from the `Canonical Data Models` section. This is a core, shared data structure and should be formally documented there. | **Low** | Add the `ProviderTokens` class definition to the `Canonical Data Models` section for completeness. |
| **36** | **Unclear Responsibility for `SyncHistory`:** `05-data-sync.md` mentions storing the result of an AI merge in a `SyncHistory` record to provide transparency. This is the only mention of such a concept. What is the schema of this record? Where is it stored? Who is responsible for writing it? This feature is completely undefined. | **Medium** | If `SyncHistory` is a required feature, it needs to be fully specified. This includes its data model, storage (presumably a new item type in DynamoDB), and the component responsible for creating it. If it's just an idea, it should be removed to avoid confusion. |
| **37** | **Ambiguous `pushData` Interface:** The `DataProvider` interface in `07-apis-integration.md` defines `pushData(tokens: ProviderTokens, data: List<CanonicalData>)`. The `data` parameter is a list of a sealed interface. This implies a single push call could contain heterogeneous data types (e.g., steps and workouts). Is this intended? Most provider APIs are not designed for such batch operations. | **Medium** | Clarify if `pushData` is intended to handle lists of mixed data types. A simpler and more realistic interface might be `pushData(tokens: ProviderTokens, data: List<CanonicalWorkout>)`, with separate methods for separate data types. |
| **38** | **Missing Information: S3 Bucket Naming and Region:** The security section (`19-security-privacy.md`) details security settings for the data export S3 bucket but doesn't specify its name or in which region it will be created, which is important in a multi-region setup. | **Low** | Specify the naming convention and regional strategy for the data export S3 bucket. |
| **39** | **Ambiguous `CanonicalWorkout` Fields:** The `CanonicalWorkout` class has `durationSeconds: Double` and `distanceMeters: Double?`. Why is duration a `Double`? Durations are typically integers or `Long`. Using `Double` for seconds risks floating-point inaccuracies. | **Low** | Change `durationSeconds` from `Double` to `Long` to avoid floating-point precision issues and better represent the data type. |
| **40** | **Incomplete Chaos Engineering Plan:** The chaos engineering catalog in `06-technical-architecture.md` is a great start, but it's missing a key experiment: regional failover for the cache. The document states ElastiCache is independent per region. The plan should include an experiment to simulate a regional cache failure to test the "cache stampede" scenario. | **Medium** | Add a "Regional Cache Failure" experiment to the chaos engineering catalog to explicitly test the system's resilience to a cold cache in a failover scenario. |
| **41** | **Missing Information on `notes` field:** The PII stripping table in `06-technical-architecture.md` specifies that a `notes` field on `CanonicalWorkout` must be removed. However, the `CanonicalWorkout` data class definition in the same document does not contain a `notes` field. | **Low** | Add the `notes: String? = null` field to the `CanonicalWorkout` data class to match the anonymization table, or remove the `notes` row from the table if it's not a required field. |
| **42** | **Ambiguous `Historical Sync Job` Item Mitigation:** The document notes that storing many `HISTORICAL` items can degrade performance and suggests client-side filtering. This is a weak mitigation. It relies on all clients implementing the filter correctly. | **Medium** | Propose a stronger, backend-enforced mitigation. For example, consider storing the historical job pointers with a different partition key prefix or in a separate table altogether if they are numerous and rarely queried with the main profile data. |
| **43** | **Unclear Purpose of `jobId` in `POST /v1/sync-jobs` Response:** The API returns a `jobId`. What is this ID? Is it the idempotency key? Is it a new ID generated by the backend? Can the client use this ID to query the status of the job later? The purpose and lifecycle of this `jobId` are undefined. | **Medium** | Define the `jobId`. It is recommended that this be the same as the `Idempotency-Key` provided by the client, making the operation traceable end-to-end with a single identifier. Also, specify if there will be a `GET /v1/sync-jobs/{jobId}` endpoint to check the status. |
| **44** | **Missing API Endpoint for Re-authentication:** The application flow frequently mentions that a user needs to re-authenticate a connection. However, there is no API endpoint defined for initiating this flow. How does the client tell the backend which connection to re-authenticate? | **Medium** | Define a new API endpoint, for example `POST /v1/connections/{connectionId}/reauth`, that the client can call to get a new provider authorization URL, initiating the re-authentication flow for a specific connection. |
| **45** | **Incomplete `DataProvider` Interface:** The `DataProvider` interface in `07-apis-integration.md` is missing a method to handle token revocation. The credential lifecycle section (`19-security-privacy.md`) states the backend calls the provider's `revoke` endpoint, but the interface has no such method. | **Medium** | Add a `revoke(tokens: ProviderTokens)` method to the `DataProvider` interface to standardize how token revocation is handled across all providers. |
| **46** | **Missing Information on `AnonymizerProxy` Compute:** The `AnonymizerProxy` is a critical component for privacy. The document specifies it's a Lambda function but provides no details on its expected performance, memory requirements, or runtime (e.g., Python, TS). | **Medium** | Provide more implementation details for the `AnonymizerProxy` Lambda, including estimated resource requirements and choice of runtime, and add its latency to the overall performance budget. |
| **47** | **Ambiguous "Hot User" Identification:** The "hot user" mitigation strategy relies on a CloudWatch Alarm on the `ThrottledRequests` metric. The document states "The `userId` causing the throttling will be extracted from the logs or metrics". This is hand-wavy. How? This is not a trivial operation. CloudWatch metrics do not provide this level of dimensionality by default. | **High** | This is a critical gap in the "hot user" strategy. Specify the exact mechanism for identifying which `userId` is causing a hot partition. This likely requires creating high-cardinality custom metrics or a complex log parsing solution, both of which have significant cost and implementation implications. |
| **48** | **Unclear `PushResult` Handling:** The `pushData` method returns a `PushResult` with `failedItemIds`. The document says for the MVP, the entire job will fail if this list is not empty. This is clear. But what is the expected format of an "item ID"? Is it the `sourceId` from the canonical model? This needs to be defined. | **Low** | Specify what identifier should be used in the `failedItemIds` list. Using the `sourceId` from the canonical model is the most logical choice. |
| **49** | **Missing Information on Kinesis Firehose Configuration:** The document proposes Kinesis Firehose for analytics ingestion. However, it provides no details on its configuration, such as buffering hints (e.g., buffer size, buffer interval), compression format (e.g., GZIP, Snappy), or the partitioning strategy for S3 delivery. | **Medium** | Provide key configuration parameters for the Kinesis Data Firehose stream to ensure it is set up for cost-effectiveness and query performance on S3. |
| **50** | **Ambiguous `maxReceiveCount` for SQS:** The documents mention SQS redrives messages to the DLQ after `maxReceiveCount` is exceeded, but the value for `maxReceiveCount` is never specified. This is a critical tuning parameter for reliability. | **Medium** | Specify a default `maxReceiveCount` for the primary SQS queue (e.g., 5). Justify the choice as a balance between allowing recovery from transient errors and not waiting too long to detect a persistent failure. |
| **51** | **Missing Information on AppConfig Validators:** The document praises AWS AppConfig for its validation capabilities but doesn't specify any validators to be used. For critical configurations like the DynamoDB table name, a validator (e.g., a regex or a Lambda function) should be mandatory to prevent typos from causing an outage. | **Medium** | For critical configurations managed by AppConfig, specify that a corresponding validator **must** be implemented. For example, a regex validator for the DynamoDB table name ARN format. |
| **52** | **Unclear Ownership of `correlationId` Generation:** Multiple documents refer to the `correlationId` for tracing. Who generates it? Is it the mobile client? The API Gateway? The Lambda Authorizer? The Powertools library? | **Medium** | Specify the single, definitive source of truth for `correlationId` generation. The recommended location is at the very beginning of the request lifecycle, either in API Gateway or, more practically, by the Powertools library in the `AuthorizerLambda`. |
| **53** | **Missing Information: `CanonicalSleepSession` Usage:** The document defines a `CanonicalSleepSession` model but never describes how or where it's used. No API endpoints or sync jobs mention sleep data. | **Low** | If sleep data is in scope for the MVP, add it to the relevant API contracts and sync job descriptions. If it is not in scope, move the `CanonicalSleepSession` model to an appendix or a "Future" section to avoid confusion. |
| **54** | **Ambiguous `Account Deletion` Flow:** The deletion workflow in `19-security-privacy.md` says it revokes tokens and then deletes DynamoDB data. What happens if deleting the DynamoDB data fails after the tokens have been successfully revoked? The user is left in an inconsistent state. | **Medium** | The account deletion process should be made more robust. It should be idempotent and handle partial failures. For example, it could first mark the user as "deleting" in DynamoDB, then attempt all revocations and deletions, and only at the very end remove the user record. |
| **55** | **Missing Client-Side Performance Metrics:** The performance budget in `16-performance-optimization.md` is excellent for the backend but is missing key client-side metrics like binary size, crash-free session rate, and offline responsiveness. | **Medium** | Add key client-side performance and quality metrics to the performance budget table, such as "App Binary Size", "Crash-Free Session Rate", and "Time to load dashboard while offline". |
| **56** | **Unclear Diagram: `State Machine for a Sync Job`:** The diagram in `05-data-sync.md` is titled "State Machine for a Sync Job" but it actually depicts the lifecycle of a message in SQS, not a Step Functions state machine. This is confusing. | **Low** | Rename the diagram to more accurately reflect its content, for example: "Lifecycle of a Real-time Sync Job Message in SQS". |
| **57** | **Incomplete `Technology Radar`:** The technology radar is a great idea but seems incomplete. For example, it lists `Pact` for contract testing but doesn't mention the UI testing frameworks (`Espresso`/`XCUITest`) or mocking libraries (`MockK`) mentioned elsewhere. | **Low** | Flesh out the Technology Radar to include all major technologies and libraries mentioned throughout the documents for consistency. |
| **58** | **Ambiguous `AI-Powered Merge` User Transparency:** `05-data-sync.md` says the client will display an "informational icon" if the AI merge fails and falls back to the default. What does this icon look like? What is the exact text of the tooltip? This UX detail is important for the feature's transparency. | **Low** | Provide a more detailed UX specification for the AI merge fallback indicator, or link to a separate UX design document. |
| **59** | **Missing Information: `DELETE /v1/user/me` is Asynchronous:** The user deletion process involves multiple network calls and could take time. The API should not be synchronous. The `DELETE /v1/user/me` endpoint should return a `202 Accepted` and queue an asynchronous job to perform the deletion. | **Medium** | Redefine the `DELETE /v1/user/me` endpoint to be asynchronous, returning a `202 Accepted` response immediately and queuing the deletion workflow on the backend. This provides a better and more reliable user experience. |
| **60** | **Missing `dataType` in `SYNCCONFIG` Example:** `06-technical-architecture.md` shows an example `SYNCCONFIG` SK as `SYNCCONFIG#fitbit#to#googlefit#steps`. It correctly includes the `dataType`. However, the table definition for the `SYNCCONFIG` item's SK is `SYNCCONFIG#{sourceId}#to#{destId}`. It's missing the `#{dataType}` component. | **High** | The `SYNCCONFIG` Sort Key format in the table definition is incorrect and must be updated to `SYNCCONFIG#{sourceId}#to#{destId}##{dataType}` to match the example and the required access patterns. This is a critical data modeling error. |
| **61** | **Ambiguous "Source of Truth" for Historical Sync:** The `06-technical-architecture.md` document states for the `Hist. Sync Job` item in DynamoDB, "The definitive status is stored in the state machine itself." This is good. But `05-data-sync.md` says the mobile app can query a backend API that uses `DescribeExecution` to get the status. This suggests the app polls for status. This is inefficient. | **Medium** | For a better user experience, the system should use push notifications to inform the user when a historical sync is complete or has failed. The final step of the Step Functions state machine should publish an event to trigger notification `N-05` or `N-06`. Relying on client polling is inefficient. |
| **62** | **Missing Information on Test Data Management:** The `Developer Experience` section mentions resetting DynamoDB data for test accounts. How is this achieved? With a script? Who can run it? This is a critical part of the CI/CD process and is undefined. | **Medium** | Provide a detailed specification for the test data management and reset process. This should include the script or mechanism used, and the IAM permissions required to perform the reset. |
| **63** | **Unclear Fallback for `Trial Ending` Notification:** `29-notifications-alerts.md` specifies a backend fallback for the "Trial Ending" notification. How does the backend know if the user has "not yet seen" the locally scheduled notification? There is no mechanism described for the client to report this back to the server. | **Medium** | This fallback logic is unimplementable as specified. Either remove the fallback, or add a mechanism for the client to report to the backend that the local notification has been successfully displayed to the user. |
| **64** | **Ambiguous `GPS Data` Anonymization:** The PII stripping table in `06-technical-architecture.md` says to "Remove" GPS data, with the note "(If added in future)". This is ambiguous. Does the canonical model support GPS data or not? | **Low** | Make a decision on whether GPS data is in scope. If so, add it to the `CanonicalWorkout` model. If not, remove the row from the anonymization table to avoid confusion. |
| **65** | **Missing Information on CI/CD for `DLQAnalyzer`:** The `DLQAnalyzer` Lambda is a critical piece of operational automation. The documents do not mention how it is configured or deployed. Is it part of the main backend CI/CD? How are its "known error patterns" updated? | **Medium** | Specify the deployment and configuration management strategy for the `DLQAnalyzer`. The known error patterns should be stored in a manageable format (e.g., a JSON file in S3) and updated via a safe deployment process. |
| **66** | **Incomplete `POST /v1/export-jobs` API:** The data export API returns a `jobId` but there is no corresponding `GET /v1/export-jobs/{jobId}` endpoint for the client to check the status of the export. The user is left waiting for a push notification with no way to proactively check the status. | **Medium** | Add a `GET /v1/export-jobs/{jobId}` endpoint to the API contract to allow clients to poll for the status of a data export job. |
| **67** | **Ambiguous "Hot Path" vs "Real-time Sync":** The documents use the terms "Hot Path" and "Real-time Sync" interchangeably. While they refer to the same thing, standardizing on a single term would improve clarity. | **Low** | Choose one term ("Hot Path" is more descriptive of the architecture) and use it consistently across all documents. |

---

## 4. Technical Feasibility Assessment

This section assesses the practicality of the proposed implementation, focusing on technology choices, performance goals, and potential integration complexities.

| ID | Description | Impact/Severity | Recommendation |
| :--- | :--- | :--- | :--- |
| **68** | **CRITICAL: Feasibility of 45,000 Concurrent Lambdas:** The projection of needing **45,000 concurrent Lambda executions** is financially and technically extreme. The document correctly identifies this as a risk but understates the severity. This level of concurrency may not be provisionable in all regions, has massive cost implications, and puts extreme stress on downstream dependencies. | **Critical** | The project should be halted until a detailed cost model is created and approved. Furthermore, a proof-of-concept load test to validate this level of concurrency against dependencies (especially third-party APIs) is a mandatory prerequisite, not a post-design task. The architecture should be re-evaluated to find ways to reduce concurrency (e.g., batching, Fargate). |
| **69** | **Performance of `AuthorizerLambda`:** The `AuthorizerLambda` must be fast. The plan to write it in TS/Python is good. However, it is also responsible for writing to the `SyncWellBreakGlassIndex` DynamoDB table. This adds a synchronous network call (a database write) to the most latency-critical part of the API request path. | **High** | Has the latency of this DynamoDB `PutItem` operation been factored into the P99 latency budget for the authorizer? This write should be made asynchronously if possible (e.g., by publishing an event) to avoid adding latency to the user's API request. |
| **70** | **Scalability of Manual "Hot User" Mitigation:** The strategy for handling a "hot user" relies on an engineer manually setting a flag in the database after receiving an alert. This is not a scalable or reliable process for a system with a 1M DAU target. An alert at 3 AM should not require a manual database edit to keep the service healthy. | **High** | The "hot user" mitigation process **must be automated**. The CloudWatch alarm should trigger a Step Functions workflow that automatically performs the migration to the dedicated "hot table" and sets the flag. Manual intervention should be the exception, not the rule. |
| **71** | **Feasibility of Cross-Cloud Notification Architecture:** The notification system proposes a cross-cloud bridge from AWS SNS to a Firebase Cloud Function for dispatching push notifications. This introduces significant operational complexity, latency, and a second point of failure. | **Medium** | Justify this cross-cloud architecture more thoroughly. A simpler approach is to have an AWS Lambda function call the FCM API directly via HTTP. The document should analyze the trade-offs of this simpler approach (e.g., managing FCM credentials in AWS Secrets Manager) versus the proposed complex one. |
| **72** | **Extreme Risk of Manual Account Recovery Process:** The runbook in `18-backup-recovery.md` for manually migrating a user's data is exceptionally dangerous. It involves an engineer running a script that performs multiple, destructive read/write/delete operations on the production database. The potential for human error (e.g., wrong `userId`, script bug) leading to permanent data loss for the wrong user is immense. | **Critical** | A fully manual, script-based process for this operation is unacceptable for a production system. This process **must** be implemented as a purpose-built, audited, and peer-reviewed internal tool with multiple safety checks before it can be used. For the MVP, this feature should be considered unsupported. |
| **73** | **Unproven Technology: KMP on AWS Lambda:** While technically possible, running Kotlin Multiplatform (compiled to a JAR) on AWS Lambda for the core backend logic is an uncommon architectural choice. The document acknowledges the cold start problem but may be underestimating other operational friction points, such as debugging, library compatibility, and the developer feedback loop. | **Medium** | A technical spike or proof-of-concept should be completed to validate the KMP-on-Lambda approach. This PoC should measure cold start times with the full application context, verify the developer workflow with LocalStack, and confirm there are no critical library incompatibilities. |
| **74** | **Dependency on Unofficial Garmin API:** The endpoint listed for Garmin (`daily-summary-service/...`) appears to be from an unofficial, reverse-engineered API, not a formal, documented developer API. Relying on an unofficial API is extremely risky, as it can change or be shut down without notice. | **High** | Clarify the official status of the Garmin API being used. If it is not an official, supported developer API, Garmin integration must be flagged as a high-risk feature that could break at any time, and a backup plan should be considered. |
| **75** | **Complexity of the `DataProvider` SDK:** The plan to build a versioned, packaged `DataProvider` SDK, while good practice, adds significant overhead for the MVP. It requires setting up a private artifact repository, managing a release process for the SDK itself, and coordinating dependency updates. | **Low** | For the MVP, strongly consider the document's own suggestion of keeping the `DataProvider` interfaces as a simple shared module within the main application's monorepo. This reduces initial complexity and increases development speed. The full SDK can be extracted later if the number of integrations grows significantly. |
| **76** | **Realism of P90 Worker Duration SLO:** The `Worker Lambda Duration` SLO is set to `< 15 seconds` (P90). Given that a single sync job can involve multiple, sequential API calls to slow third-party services, this target may be difficult to achieve consistently. | **Medium** | Validate this SLO with a proof-of-concept that calls the real third-party APIs for the MVP providers. The results should be used to set a more realistic performance target. The document should also differentiate between the worker's own processing time and the time spent waiting on external networks. |

---

## 5. Risk and Gap Analysis

This section identifies potential risks (Security, Operational, etc.) and unaddressed areas ("unknown unknowns") in the architecture.

| ID | Category | Description | Impact/Severity | Recommendation |
| :--- | :--- | :--- | :--- | :--- |
| **77** | Security | **Risk of `ProviderTokens` Serialization:** The `ProviderTokens` data class in `06-technical-architecture.md` is marked with Kotlin's `@Serializable` annotation. If an instance of this class is ever logged or serialized to a non-secure location, it could leak user access and refresh tokens in plain text. | **Critical** | This is a critical security vulnerability waiting to happen. The `ProviderTokens` class **must not** be generally serializable. If serialization is required, custom serializers that explicitly omit the token fields must be used. Furthermore, its `toString()` method must be overridden to redact the token values to prevent them from appearing in logs. |
| **78** | Security | **Slow "Break-Glass" Approval Process:** The "break-glass" procedure in `19-security-privacy.md` requires a pull request review for approval. During a live incident, this process could be too slow, as the approving engineer may not be immediately available. | **Medium** | While the four-eyes principle is good, the implementation is high-friction. For the future, the planned internal tool should have a more streamlined approval flow (e.g., a PagerDuty notification with "Approve/Deny" buttons for the on-call engineer). For the MVP, the risk of a slow manual process is accepted but should be acknowledged. |
| **79** | Security | **Social Engineering Risk in Manual Account Recovery:** The identity verification steps for manual account recovery (subscription receipt, questions about config) are potentially vulnerable to social engineering. A determined attacker could potentially gather this information and impersonate a user to take over their account. | **High** | The identity verification process must be strengthened. For the MVP, it might be safer to declare that account recovery is not possible if the user loses access to their sign-in method. If the feature is required, it needs a much more robust identity verification flow. |
| **80** | Security | **Lack of Granularity in `AuthorizerLambda` Caching:** The authorizer caches the IAM policy based on the user's identity token for 5 minutes. If a user's permissions change (e.g., they are demoted from a "Pro" plan), they will retain their old permissions for up to 5 minutes. | **Low** | This is a reasonable trade-off for performance, but it must be explicitly documented as an accepted security risk. For highly sensitive operations, consider adding a specific, non-cached authorizer. |
| **81** | Security | **VPC Endpoints for S3 are Gateway, not Interface:** The document states VPC Endpoints will be used for S3. It should clarify that for S3, these are "Gateway Endpoints," which are free, whereas for other services like SQS, they are "Interface Endpoints," which have an hourly cost. The cost difference is significant. | **Low** | Clarify the type of VPC Endpoint used for each service to ensure accurate cost modeling. |
| **82** | Security | **Missing Security Review for Open-Source Tools:** The plan mentions using several open-source tools like `n8n`, `crewAI`, and `LangGraph`. It's unclear if these tools have undergone a proper security review. | **Medium** | Mandate a security review and threat modeling exercise for any open-source AI framework before it is integrated into the system, especially for the `Interactive AI Troubleshooter` feature. |
| **83** | Operational | **High Maintenance Burden of `DLQAnalyzer`:** The `DLQAnalyzer` in `17-error-handling.md` needs to be constantly updated with "known error patterns." As new providers are added and existing APIs change, this will become a significant and ongoing maintenance task for the engineering team. | **Medium** | Acknowledge the operational cost of maintaining the `DLQAnalyzer`. A long-term strategy should be to make `DataProviders` more resilient so they fail less often, reducing reliance on the analyzer. |
| **84** | Operational | **Lack of Third-Party API Versioning Strategy:** The `DataProvider` architecture does not account for versioning of the third-party APIs it integrates with. If Strava releases a breaking v4 of their API, how is the `StravaProvider` updated? Do we support both v3 and v4 simultaneously? This is a critical gap in the integration strategy. | **High** | The `DataProvider` architecture must be extended to include a concept of API versioning. For example, a `StravaV3Provider` and a `StravaV4Provider` could co-exist, and the system would choose the correct one based on user settings or a migration flag. |
| **85** | Operational | **Undefined CI/CD Environment Strategy:** The documents mention a CI/CD pipeline but do not detail the environment strategy. Is there a `development` environment separate from `staging`? How do developers test against sandboxed versions of third-party APIs? | **Medium** | Define the full environment strategy, including `development`, `staging`, and `production`. Specify the process for provisioning test accounts and data in each environment, including how third-party API sandboxes will be utilized. |
| **86** | Gap | **Data Residency and GDPR:** The architecture proposes using DynamoDB Global Tables, which replicate data across multiple geographic regions (e.g., US, EU, AP). This could violate data residency requirements of regulations like GDPR, which may mandate that EU citizens' data remains within the EU. | **Critical** | This is a major compliance gap. A data residency strategy must be defined. This might involve abandoning Global Tables in favor of a siloed, region-specific architecture, which would be a fundamental change to the entire high-availability plan. Consult with legal and compliance experts immediately. |
| **87** | Gap | **No Exit Strategy for Firebase Authentication:** The document accepts the hard dependency on Firebase Authentication. However, it does not discuss an exit strategy. What happens if Google significantly changes the pricing or terms of service? The cost and effort to migrate 1M users to a different identity provider would be immense. | **High** | While migrating away would be difficult, the document should at least outline a high-level plan or acknowledgment of what an exit strategy would entail. This demonstrates long-term thinking beyond the MVP. |
| **88** | Gap | **No Strategy for Data Back-filling for New Providers:** The documents do not discuss the business or technical strategy for handling data back-filling when a new provider is added. If "Polar" is added as a new provider, will existing users be offered a historical sync? This could cause a massive, unpredictable load spike on the system. | **Medium** | Define the policy for historical data back-filling for newly added providers. If it will be offered, the potential load impact must be modeled and factored into the scalability and cost plans. |
| **89** | Gap | **No Client-Side API Versioning Strategy:** The documents mention versioning the backend API (e.g., `/v1/`, `/v2/`) but do not specify how the mobile client will handle this. Is there a forced update mechanism for the app? How long will older versions of the API be supported? | **Medium** | Define the mobile application's strategy for handling backend API version changes. This should include a plan for graceful degradation, forced updates, and a deprecation policy for old API versions. |
| **90** | Gap | **Missing Strategy for Managing API Quotas:** The document focuses on rate limits (requests per second) but doesn't mention quotas (e.g., 10,000 calls per day). Some APIs have daily or monthly quotas. The rate-limiting system is not designed to handle these long-term quotas. | **Medium** | The integration analysis for each provider must include details on long-term quotas. If providers have such quotas, a separate mechanism for tracking and managing quota usage is required to prevent being cut off mid-month. |
| **91** | Gap | **No Mention of Caching Negative Lookups:** The caching strategy focuses on caching found items ("cache hits"). It does not mention caching negative lookups (i.e., when an item is confirmed not to exist in the database). Caching "not found" responses can be an important optimization to prevent repeated, fruitless database queries. | **Low** | The caching strategy should be updated to include caching of negative results (e.g., storing a special "not-found" value for a short TTL) to further reduce database load. |
| **92** | Gap | **No Strategy for Schema Migrations:** The document specifies using AWS Glue Schema Registry for data models, which is excellent. However, it doesn't describe the process for handling a breaking change. If a non-backward-compatible change is required, how is the migration handled? | **Medium** | Define a strategy for handling breaking schema changes. This is a complex process that might involve a "two-phase" deployment, where the system temporarily supports both the old and new schemas to allow for a gradual migration. |
| **93** | Gap | **Missing Alerting on Idempotency Key Collisions:** The document details the idempotency mechanism but does not specify any monitoring on it. A sudden spike in idempotency key "hits" could indicate a critical bug in the client's key generation logic, leading to silent failures for users. | **High** | A custom CloudWatch metric and alarm **must** be created to monitor the rate of idempotency key collisions. An alert on this metric would be an early warning sign of a serious client-side bug. |
| **94** | Security | **Insufficient Peer Review for Critical Scripts:** The "break-glass" and manual recovery scripts require peer review via pull request. This is a good baseline, but for operations that can cause permanent data loss, a simple PR approval may not be sufficient. | **High** | For the most critical, destructive operational scripts, consider implementing a more robust approval process. For example, the script could be designed to require two authorized engineers to enter their MFA codes simultaneously to execute (a "two-person rule"). |
| **95** | Operational | **No Plan for LocalStack in CI/CD:** The document correctly identifies LocalStack for local development. However, it's not mentioned as part of the CI/CD pipeline. Running integration tests against LocalStack in the CI pipeline is a best practice for ensuring backend components work together before deploying to a real AWS environment. | **Medium** | The CI/CD strategy should be updated to include a stage where backend integration tests are run against a LocalStack instance. This provides a faster and cheaper feedback loop than deploying to a full `dev` or `staging` environment. |
| **96** | Gap | **No Mention of Timezones:** The canonical data models use ISO 8601 timestamps, which is great for UTC. However, the system deals with health data, which is often timezone-dependent (e.g., "sleep from 10 PM to 6 AM"). The canonical models do not have a field for the original timezone, which could lead to incorrect calculations or display issues. | **High** | The canonical data models (especially `CanonicalSleepSession`) must be updated to include the original timezone identifier (e.g., `tz="America/New_York"`). Storing this is critical for correctly interpreting user health data. |
| **97** | Security | **Unsafe Use of `CredentialArn`:** The data model stores the `CredentialArn` from AWS Secrets Manager in the `Connection` item in DynamoDB. This ARN can be predictable and could leak information about the secret's naming convention. | **Low** | While not a critical vulnerability, a better practice is to store a randomly generated UUID as the pointer to the secret in the database. The application would then use this UUID to look up the full ARN from a secure mapping, adding a layer of indirection. |
| **98** | Operational | **No Strategy for Retiring a `DataProvider`:** The documents focus on adding new providers, but not on retiring them. What is the process if a third-party service shuts down or we decide to end an integration? | **Medium** | Define a formal process for retiring a `DataProvider`. This should include notifying affected users, disabling the sync jobs, and a plan for handling the now-obsolete user connections and credentials. |
| **99** | Gap | **Missing Definition of "Pro" vs. Free Tiers:** The documents mention "Pro" features like `AI-Powered Merge`. However, there is no comprehensive list of which features are free and which are part of the Pro subscription. This is critical for both development and marketing. | **Medium** | Create or reference a dedicated document that clearly defines the feature set for each subscription tier (Free, Pro, etc.). |
| **100** | Clarity | **Overloaded Term "Worker":** The term "worker" is used to refer to the Lambda function that processes sync jobs. However, Step Functions also has a concept of "workers". In the context of the "Cold Path", it's sometimes unclear if "worker" refers to the Lambda function or a component of Step Functions. | **Low** | To improve clarity, use more specific terms. For example, "Worker Lambda" for the compute function and "State Machine" or "Orchestrator" for Step Functions. |
| **101** | Security | **Potential for Log Injection:** The document mandates structured JSON logging, which is good. However, it does not explicitly mention sanitizing user-provided data before it is logged. A malicious user could potentially inject fake JSON structures into fields like the `title` of a workout, which could break log parsing systems. | **Medium** | Mandate that all user-provided input must be sanitized or properly escaped before being included in any structured log entry to prevent log injection vulnerabilities. |
| **102** | Gap | **No Mention of Feature Flag Cleanup:** The document praises the use of AWS AppConfig for feature flags. However, it does not define a process for removing old or obsolete feature flags from the code and configuration. Over time, this can lead to significant technical debt. | **Low** | Establish a formal process for the lifecycle management of feature flags, including a policy for removing them once a feature is fully rolled out or deprecated. |
| **103** | Operational | **No Alert on Successful Regional Failover:** The HA plan details how Route 53 will automatically fail over. However, it does not specify that this event should trigger an alert. A regional failover is a major event, and the on-call team should be immediately notified that it has happened, even if it was successful. | **Medium** | Configure a CloudWatch Alarm on the Route 53 health check status to send a high-priority notification (e.g., to a Slack channel) whenever a regional failover event occurs. |
| **104** | Security | **Insufficient Security on Data Export Bucket:** The S3 bucket for data exports has good baseline security but is missing a critical control. The pre-signed URL grants access to whoever holds it. If a user's email is compromised, an attacker could trigger an export and get the link. | **High** | The data export process should require re-authentication within the app before the download link is shown. The push notification should simply state that the export is ready and prompt the user to open the app, not include the download link directly. |

---
*End of Review*
