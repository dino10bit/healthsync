# Production Readiness & Cost-Model Review: SyncWell

**Reviewer:** Jules (Principal Architect, Staff SRE, Security Engineer)
**Date:** 2025-08-22
**Subject:** Production-readiness review and cost-model reconciliation for the SyncWell application at 1,000,000 DAU.
**Input Documents:**
- `06-technical-architecture.md` (v1.3)
- `66-costs-model.md`

---

## 1. Executive Summary

This review finds that the SyncWell architecture (`06-technical-architecture.md`) is exceptionally well-designed but requires specific, material hardening to be production-ready for 1,000,000 DAU. The reconciled cost model (`66-costs-model.md`) is directionally correct but understates the financial risk associated with network egress and observability, which will be the dominant cost drivers at scale. The architecture's heavy reliance on custom, fine-grained logic (e.g., tiered logging, metadata-first hydration) is both its greatest strength and its biggest risk; failure to implement these patterns correctly will lead to significant cost overruns.

*   **BLOCKER #1: Undefined Traffic & Payload Assumptions.** The entire capacity plan and cost model are based on assumed traffic patterns. The highest priority is to validate these assumptions with a private beta before a full public launch.
*   **DEFECT #2: Incomplete Cost Model.** The cost model correctly revises the compute estimate but significantly underestimates network egress and observability costs, which will likely account for >40% of the monthly bill at scale. The model must be updated to reflect this.
*   **DEFECT #3: Insufficient DR/HA for Critical Dependencies.** The architecture specifies multi-AZ for its core components but does not adequately address the disaster recovery posture for critical external dependencies, chiefly the single-region Firebase Authentication service.
*   **GAP #4: Lack of Granular Cost Controls for AI.** The architecture mentions future AI/ML services but lacks a concrete chargeback or cost-control model. AI services can have highly variable, non-linear cost curves, and a strategy to attribute and cap these costs is required before implementation.
*   **GAP #5: Missing API Security Controls.** The architecture specifies WAF but omits critical API security controls like per-user rate limiting and granular abuse detection, which are non-negotiable for a public-facing API at this scale.
*   **GAP #6: Immature CI/CD and Testing Strategy.** The documentation mentions testing and CI/CD but lacks the specific, concrete artifacts (e.g., test data reset scripts, contract test definitions, detailed deployment pipeline) required for safe, automated releases.

**Readiness Score: 65/100.** The system is architecturally sound but not yet production-ready. The score is justified by the critical gap between the assumed and actual cost drivers and the missing operational and security controls required to manage the platform safely at scale.

---

## 2. Concrete Assumptions & Traffic Model

The existing documentation lacks a formal, bottom-up traffic model. This is a **BLOCKER** as it makes accurate capacity planning and cost forecasting impossible. The following model provides three scenarios (Conservative, Nominal, Aggressive) to bound the problem. The **Nominal** scenario is used for all subsequent calculations in this review.

**BLOCKER - Missing Data:** The following assumptions must be validated with real-world data from a private beta before public launch.

### 2.1. Core User Activity Assumptions

| Metric | Conservative | **Nominal (Used for Review)** | Aggressive | Unit | Rationale / Source |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Daily Active Users (DAU) | 1,000,000 | **1,000,000** | 1,000,000 | Users | Target specified in prompt. |
| User Sessions / Day | 2 | **3** | 5 | Sessions/User | Industry standard for a health/fitness app. |
| API Requests / Session | 10 | **15** | 25 | Requests/Session | Assumes a mix of foreground/background activity. |
| "Hot Path" Syncs / User / Day | 5 | **10** | 20 | Syncs/User/Day | Includes automatic, webhook, and manual syncs. |
| Write/Read Ratio (DB) | 1:2 | **1:1** | 2:1 | Ratio | Nominal assumes each sync reads config then writes state. |
| Peak-to-Average Ratio | 3:1 | **5:1** | 8:1 | Ratio | A 5x peak is standard for global apps with daily usage patterns (e.g., morning/evening spikes). |
| Avg. API Payload Size | 2 KB | **5 KB** | 10 KB | Kilobytes | **BLOCKER:** This is a critical assumption for egress costs. Needs validation. |
| Avg. "Heavy" Payload Size | 250 KB | **500 KB** | 1 MB | Kilobytes | For payloads like workout GPX tracks. |
| % Heavy Payloads | 5% | **10%** | 20% | Percentage | Assumes 1 in 10 syncs involves a "heavy" payload. |

### 2.2. Capacity Calculations (Derived from Nominal Assumptions)

Here we derive the core capacity requirements based on the **Nominal** scenario.

#### **API Request Load**
*   **Total Daily Requests:** 1,000,000 DAU * 3 sessions/day * 15 requests/session = **45,000,000 requests/day**
*   **Average Requests Per Second (RPS):** 45,000,000 / (24 * 3600) = **~520 RPS**
*   **Peak Requests Per Second (RPS):** 520 RPS * 5 (Peak/Avg Ratio) = **2,600 RPS**
    *   *Sanity Check:* This aligns with the NFR of 3,000 RPS specified in `06-technical-architecture.md`.

#### **"Hot Path" Sync Job Load**
*   **Total Daily Syncs:** 1,000,000 DAU * 10 syncs/day = **10,000,000 syncs/day**
*   **Average Syncs Per Second:** 10,000,000 / (24 * 3600) = **~115 syncs/sec**
*   **Peak Syncs Per Second:** 115 syncs/sec * 5 (Peak/Avg Ratio) = **~575 syncs/sec**

#### **Database Query Load (DynamoDB QPS)**
*   **Assumption:** Each sync job performs 1 read (e.g., get config) and 1 write (e.g., update state).
*   **Average Read QPS:** 115 syncs/sec * 1 read/sync = **115 R-QPS**
*   **Average Write QPS:** 115 syncs/sec * 1 write/sync = **115 W-QPS**
*   **Peak Read QPS:** 575 syncs/sec * 1 read/sync = **575 R-QPS**
*   **Peak Write QPS:** 575 syncs/sec * 1 write/sync = **575 W-QPS**
    *   *Note on Global Tables:* With a Global Table for DR, the effective write QPS for billing purposes is doubled to **1,150 W-QPS** at peak.

#### **Cache Throughput (Redis)**
*   **Assumption:** The cache handles config reads, rate limiting, and session data. We'll model this as 2 cache operations per sync.
*   **Peak Cache Ops/Sec:** 575 syncs/sec * 2 ops/sync = **1,150 ops/sec**
    *   *Sanity Check:* The architecture doc specifies a target of 5,000 RPS for the cache, which provides a healthy >4x headroom.

#### **Network Egress Bandwidth**
This is a critical, cost-driving calculation that was previously underestimated.
*   **Light Payload Egress:**
    *   90% of syncs are "light": 10,000,000 * 0.90 = 9,000,000 syncs/day
    *   Data per day: 9,000,000 syncs * 5 KB/sync = 45,000,000 KB = **45 GB/day**
*   **Heavy Payload Egress:**
    *   10% of syncs are "heavy": 10,000,000 * 0.10 = 1,000,000 syncs/day
    *   Data per day: 1,000,000 syncs * 500 KB/sync = 500,000,000 KB = **500 GB/day**
*   **Total Daily Egress:** 45 GB + 500 GB = **545 GB/day**
*   **Total Monthly Egress:** 545 GB/day * 30 days = **16,350 GB/month = ~16 TB/month**
    *   **Conclusion:** This is nearly **2x higher** than the reconciled estimate in `66-costs-model.md` and **10x higher** than the original estimate. This highlights the extreme sensitivity of the cost model to payload size and is a major financial risk. The "metadata-first" hydration pattern is not just an optimization; it is a **mandatory architectural requirement** to control costs.

---

## 3. Topology & Critical-Path Architecture Changes

The existing architecture is well-structured. The following changes are prioritized recommendations to harden the system for 1M DAU, focusing on security, cost control, and availability.

### 3.1. Recommended Critical-Path Flow

This diagram illustrates the recommended end-to-end flow, incorporating multi-AZ and multi-region resilience boundaries.

```mermaid
---
title: "Recommended Critical Path (Multi-AZ & Multi-Region)"
---
graph TD
    subgraph "Global / DNS"
        User("`fa:fa-user User`")
        Route53("`fa:fa-route Route 53`")
    end

    subgraph "AWS Region: us-east-1 (Primary)"
        style `AWS Region: us-east-1 (Primary)` fill:#e6f3ff,stroke:#0073bb

        subgraph "Edge"
            CloudFront("`fa:fa-globe-americas CloudFront`")
            WAF("`fa:fa-shield-alt WAF`")
        end

        subgraph "API Layer"
            ApiGateway("`fa:fa-server API Gateway`")
            Authorizer("`fa:fa-bolt Authorizer Lambda`")
            UsagePlan("`fa:fa-tachometer-alt Usage Plan<br>(Per-User Rate Limit)`")
        end

        subgraph "Availability Zone 1"
            style `Availability Zone 1` fill:#ffffff,stroke-dasharray: 5 5
            Lambda_A("`fa:fa-bolt Worker Lambda`")
            Redis_A("`fa:fa-memory Redis Primary`")
        end

        subgraph "Availability Zone 2"
            style `Availability Zone 2` fill:#ffffff,stroke-dasharray: 5 5
            Lambda_B("`fa:fa-bolt Worker Lambda`")
            Redis_B("`fa:fa-memory Redis Replica`")
        end

        SQS("`fa:fa-inbox SQS Queue`")
        Dynamo_P["`fa:fa-database DynamoDB<br>(Primary)`"]
    end

    subgraph "AWS Region: us-west-2 (DR)"
        style `AWS Region: us-west-2 (DR)` fill:#fde4d8,stroke:#d95300
        Dynamo_DR["`fa:fa-database DynamoDB<br>(Replica)`"]
    end

    %% Connections
    User --> Route53
    Route53 -- "Latency Routing" --> CloudFront
    CloudFront --> WAF
    WAF --> ApiGateway
    ApiGateway -- "Authorizes via" --> Authorizer
    ApiGateway -- "Enforces" --> UsagePlan
    ApiGateway -- "Enqueues job" --> SQS

    SQS -- "Triggers" --> Lambda_A & Lambda_B
    Lambda_A & Lambda_B -- "Read/Write" --> Redis_A
    Redis_A -- "Replicates to" --> Redis_B
    Lambda_A & Lambda_B -- "Read/Write State" --> Dynamo_P
    Dynamo_P -- "Global Table Replication" --> Dynamo_DR
```

### 3.2. Subsystem Changes (Prioritized)

#### **1. Identity: Firebase Authentication**
*   **Problem:** The reliance on Firebase Auth as a single, non-AWS identity provider is the most significant strategic risk, creating a single point of failure outside of the team's direct control.
*   **Recommended Change:** Implement a **dual-authorizer strategy**. The API Gateway Authorizer Lambda must be modified to accept JWTs from **both** Firebase Authentication and a new, fully-configured **Amazon Cognito User Pool**.
    *   The mobile client will continue to use Firebase by default.
    *   A "break-glass" procedure will be documented to switch the client to Cognito via a configuration flag fetched from AppConfig in the event of a major Firebase outage.
*   **Capacity Config:**
    *   **Cognito User Pool:** Provisioned in `us-east-1` with a replica in `us-west-2`.
    *   **Authorizer Lambda:** 256MB RAM, ARM64, with 100 provisioned concurrency to ensure low-latency auth.
*   **Pros:** Dramatically reduces vendor lock-in and provides a credible disaster recovery path for the most critical dependency.
*   **Cons:** Increases complexity and cost. Requires maintaining two identity systems in parallel.
*   **Fallback:** Do nothing and accept the high-impact risk of a Firebase outage.

#### **2. Ingress/API Gateway: Missing Abuse Protection**
*   **Problem:** The current architecture lacks protection against a single, authenticated user overwhelming the system with a high volume of requests (e.g., a buggy client or malicious actor).
*   **Recommended Change:** Implement **per-user rate limiting** using **API Gateway Usage Plans**.
    *   Each user must be programmatically associated with their own API Key upon signup.
    *   A default Usage Plan must be configured to enforce a reasonable rate limit (e.g., **10 requests/second**) and a daily quota (e.g., **10,000 requests/day**).
*   **Capacity Config:** API Gateway handles this natively. The main effort is in the user lifecycle logic to create/manage API keys.
*   **Pros:** Provides a critical layer of defense against abuse, protecting downstream resources and controlling costs.
*   **Cons:** Adds complexity to the user signup and management process.
*   **Fallback:** Rely only on the global WAF rate limiting, which is insufficient as it cannot distinguish between individual authenticated users.

#### **3. App Compute (Lambda): Hardening for Cost & Performance**
*   **Problem:** The architecture document correctly identifies Lambda as the compute choice but lacks specific production configurations.
*   **Recommended Change:**
    1.  **Standardize on ARM64:** All Lambda functions must use the Graviton2 (`arm64`) architecture for a ~20% price-performance benefit.
    2.  **Set Provisioned Concurrency:** To meet the P99 latency SLO of <500ms for the API, the `AuthorizerLambda` and any other synchronous Lambdas connected to API Gateway must have **Provisioned Concurrency** enabled. Start with a value of **100** and tune based on traffic. For the main `WorkerLambda`, start with **50** to ensure fast manual syncs.
    3.  **Memory Configuration:**
        *   `AuthorizerLambda`: 256MB.
        *   `WorkerLambda`: **1024MB**. This is a starting point and must be tuned using AWS Lambda Power Tuning.
    4.  **Enforce Tiered Logging:** The tiered logging strategy described in the architecture is **not optional**. It is a mandatory requirement to control CloudWatch costs. This must be enforced via code reviews and specific tests.
*   **Pros:** Ensures consistent low latency for user-facing operations and optimizes the cost-performance ratio.
*   **Cons:** Provisioned Concurrency adds a fixed hourly cost.
*   **Fallback:** Use on-demand Lambda only, accepting the risk of cold-start latency impacting user experience.

#### **4. Database (DynamoDB): DR & Capacity Management**
*   **Problem:** The architecture mentions multi-region DR but needs a more explicit configuration.
*   **Recommended Change:**
    1.  **Enable DynamoDB Global Tables:** The `SyncWellMetadata` table must be configured as a Global Table, with a replica in `us-west-2`. This is the primary mechanism for meeting the <15 minute RPO.
    2.  **Enable Point-in-Time Recovery (PITR):** This is a non-negotiable requirement for disaster recovery.
    3.  **Use Hybrid Capacity Mode:** Configure the table to use **On-Demand** capacity mode. For cost optimization, purchase a **DynamoDB Savings Plan** that covers the *average* expected read/write throughput, letting On-Demand handle the peaks. This is more flexible than Provisioned Throughput.
*   **Capacity Config:**
    *   **Primary Region:** `us-east-1`
    *   **DR Region:** `us-west-2`
    *   **Savings Plan:** Purchase a plan covering **150 WCU** and **150 RCU** as a starting point (based on average QPS from the traffic model).
*   **Pros:** Provides a robust, low-RPO disaster recovery solution. The hybrid capacity model balances cost and performance.
*   **Cons:** Global Tables double the cost of every write operation.
*   **Fallback:** Rely only on PITR backups, which would result in a much higher RTO (>1 hour) during a region failure.

#### **5. Networking: Enforce VPC Endpoints**
*   **Problem:** The architecture mentions VPC endpoints but should treat them as mandatory.
*   **Recommended Change:** Create and use **VPC Gateway and Interface Endpoints** for all AWS services that the Lambda functions communicate with (S3, SQS, DynamoDB, Secrets Manager, AppConfig).
*   **Capacity Config:** N/A (managed service).
*   **Pros:**
    *   **Security:** Keeps traffic within the AWS private network, reducing exposure to the public internet.
    *   **Cost:** **This is a critical cost-optimization lever.** Data transfer to VPC endpoints is significantly cheaper than data transfer through a NAT Gateway. This will save thousands of dollars per month at scale.
*   **Cons:** Minor increase in complexity and a small hourly charge for interface endpoints (which is dwarfed by the savings).
*   **Fallback:** Route all traffic through the NAT Gateway, incurring significant and unnecessary data transfer costs.

---

## 4. Availability, Resilience & DR

The architecture's goal of >99.9% uptime is achievable but requires formalizing SLOs, DR procedures, and operational runbooks.

### 4.1. Recommended SLOs & Error Budget Policy

*   **SLOs (Service Level Objectives):**
    *   **Availability:**
        *   **Core API:** `99.95%` of requests to `/v1/` endpoints, measured over a 28-day window, return a non-5xx response.
        *   **Hot-Path Sync Success:** `99.9%` of `HotPathSyncRequested` jobs complete successfully (i.e., do not end in the DLQ).
    *   **Latency:**
        *   **API Gateway (p99):** `< 500ms` for all synchronous API calls.
        *   **Manual Sync (p95):** `< 15 seconds` from user request to success confirmation.

*   **SLIs (Service Level Indicators):**
    *   **Availability:** `(count(non-5xx responses) / count(total responses)) * 100`
    *   **Latency:** `histogram_quantile(0.99, sum(rate(api_latency_bucket[5m])))`

*   **Error Budget Policy:**
    *   An error budget is the inverse of the SLO (e.g., `100% - 99.95% = 0.05%` budget).
    *   **If 50% of the 28-day error budget is consumed in any 7-day period:** An automatic, high-severity alert is sent to the on-call lead and engineering manager. A retrospective is required.
    *   **If 100% of the 28-day error budget is consumed:** All new feature development for the affected service is frozen. Engineering efforts must be redirected to reliability improvements until the service is back within SLO.

### 4.2. RTO/RPO Targets & DR Strategy

*   **RTO (Recovery Time Objective):** `< 4 hours`. This is the target time to restore service after a declared disaster (e.g., full region outage).
*   **RPO (Recovery Point Objective):** `< 15 minutes`. This is the maximum acceptable data loss. This is primarily governed by DynamoDB Point-in-Time Recovery (PITR) and Global Table replication lag.
*   **DR Strategy:**
    *   **Primary Failure Domain:** A single AWS Availability Zone.
        *   **Mitigation:** All critical services (Lambda, ElastiCache, NAT Gateway) must be deployed in a multi-AZ configuration. An AZ failure should be a non-event with no user impact.
    *   **Secondary Failure Domain:** A full AWS Region (`us-east-1`).
        *   **Mitigation:** The primary DR strategy relies on failing over to the `us-west-2` region.
        *   **Failover Mechanics:**
            1.  **Data:** The DynamoDB Global Table replica in `us-west-2` is promoted to be the new primary.
            2.  **Traffic:** Amazon Route 53 health checks will detect the failure of the primary region's API endpoints and automatically change DNS records to route all traffic to a standby API Gateway deployment in `us-west-2`.
            3.  **Compute:** The Lambda functions and other compute resources will be deployed in the DR region (ideally via IaC) and will be activated to handle the traffic.

### 4.3. High-Level Emergency Runbooks

These are short, actionable guides for on-call engineers.

#### **Runbook 1: Sudden 5x-10x Traffic Spike**
1.  **Acknowledge Alert:** Acknowledge the CloudWatch alarm for "High API Gateway 5xx Errors" or "High Lambda Throttles".
2.  **Verify Autoscaling:**
    *   Check the CloudWatch dashboard for "Lambda Concurrent Executions". Verify that the number is scaling up to meet demand.
    *   Check the "DynamoDB Throttled Requests" metric. Verify it is zero, indicating On-Demand capacity is scaling correctly.
3.  **Identify Source:**
    *   Check the AWS WAF dashboard for anomalous traffic patterns from specific IPs or regions.
    *   Check API Gateway logs for a spike in requests to a specific endpoint or from a specific user (if per-user logging is available).
4.  **Mitigate:**
    *   **If Malicious:** Use AWS WAF to block the offending IP addresses or apply a more aggressive rate-based rule.
    *   **If Legitimate (e.g., viral event):**
        *   If Lambda is throttling, manually increase the "Provisioned Concurrency" reservation in the AWS console as a temporary measure.
        *   Post a notification to the public status page acknowledging a high-traffic event and potential for degraded performance.
5.  **Escalate:** If the situation is not controlled within 15 minutes, escalate to the Head of Engineering.

#### **Runbook 2: Primary Region (`us-east-1`) Outage**
1.  **Acknowledge Alert:** Acknowledge the Route 53 Health Check failure alarm and PagerDuty alert for "Primary Region Unresponsive".
2.  **Confirm Outage:** Verify the outage via the official AWS Status page.
3.  **Initiate Failover Procedure:**
    *   In the Route 53 console, confirm that traffic has been automatically failed over to the `us-west-2` endpoints. If not, manually trigger the failover.
    *   In the DynamoDB console, verify that the `us-west-2` replica table is now handling all read/write traffic.
    *   Verify that the Lambda functions and API Gateway in `us-west-2` are processing requests by checking their logs and metrics.
4.  **Communicate:**
    *   Update the public status page to "Major Outage" and indicate that the system is operating in a degraded state from the DR region.
    *   Notify the Head of Engineering and Product leadership.
5.  **Plan for Failback:** Once the primary region is restored (as confirmed by the AWS Status page), plan a maintenance window to fail back traffic and data replication to `us-east-1`.

#### **Runbook 3: Master Database Failure (DynamoDB Unavailability in Primary AZ)**
*Note: With DynamoDB, a "master" failure is not the right model. This runbook addresses the unavailability of the DynamoDB endpoint in a single AZ or the entire primary region.*

1.  **Acknowledge Alert:** Acknowledge the CloudWatch alarm for "High DynamoDB Read/Write Errors" or "High DynamoDB Latency".
2.  **Assess Impact:**
    *   Check if the errors are isolated to a single AZ. If so, the multi-AZ Lambda deployment should handle this gracefully by routing requests to healthy AZs. Monitor the error rate to ensure it does not escalate.
    *   Check if the errors are region-wide. This indicates a larger-scale problem.
3.  **Verify Application Behavior:**
    *   Check if sync jobs are being correctly retried and eventually sent to the DLQ.
    *   Confirm that the application is failing "open" or "closed" as expected (e.g., returning a 503 error to clients).
4.  **Trigger Manual Failover (if region-wide):**
    *   If DynamoDB is unavailable across the entire `us-east-1` region, the incident becomes a "Region Outage".
    *   **Immediately escalate to the "Primary Region Outage" runbook.** The procedure is the same: fail over DNS and data plane to the `us-west-2` DR site.
5.  **Communicate:** Keep the status page and internal stakeholders updated on the impact and mitigation steps.

---

## 5. Data, Consistency & Storage Strategy

The data strategy must ensure integrity, availability, and cost-effectiveness at scale. The single-table design proposed in the architecture is a strong foundation.

### 5.1. Data Partitioning & Indexing Strategy

*   **Partitioning Plan:** The single-table design using `PK = USER#{userId}` is the correct approach. This co-locates all data for a single user, making most queries highly efficient.
    *   **Hot Partition Risk:** As noted in the architecture doc, this design carries a risk of "hot partitions" if a single user becomes extremely active. The proposed post-MVP mitigation (migrating the user to a dedicated table) is appropriate, but a detection mechanism must be built.
    *   **Detection:** Configure a CloudWatch Contributor Insights rule on the `SyncWellMetadata` table to identify the most frequently accessed keys. An alarm should be set to trigger if a single partition key accounts for >1% of total table traffic for a sustained period (e.g., 1 hour).
*   **Indexing Strategy:** The use of a **sparse GSI** on the `ReAuthStatus` attribute is a critical and correct implementation detail for efficiently finding users who need to re-authenticate without scanning the entire table. This pattern should be considered the default for any future operational query needs.
*   **Compaction Strategy:** Not applicable for DynamoDB.

### 5.2. Backup/Restore & Snapshot Cadence

While the Global Table provides DR, a robust backup strategy is still required to protect against data corruption or catastrophic operator error.

*   **Backup Service:** **AWS Backup** must be used to centrally manage and automate all backup and retention policies for DynamoDB.
*   **Backup Cadence:**
    *   **Point-in-Time Recovery (PITR):** Must be **enabled** on the `SyncWellMetadata` table. This provides continuous backups and allows for restoration to any point in the preceding 35 days.
    *   **Daily Snapshots:** AWS Backup will be configured to take a daily snapshot of the table.
*   **Retention Policy:**
    *   **PITR backups:** Retained for **35 days**.
    *   **Daily snapshots:** Retained for **35 days**.
    *   **Monthly snapshots:** A monthly snapshot will be archived for **1 year** for compliance purposes.
*   **Restore Time Estimates:**
    *   A full restore from a snapshot for a 1TB table can take several hours. This procedure is for **catastrophic data loss**, not for HA/DR.
    *   **Restore Drills:** The SRE team must conduct a restore drill **quarterly** to validate the backup integrity and document the end-to-end restore time. The results must be logged in `../ops/dr-test-results.md`.

### 5.3. Transactional Patterns & Decision Guidance

The architecture employs several implicit transactional patterns that must be understood and managed.

*   **Idempotency:** The DynamoDB-based distributed lock (`IDEM##{key}`) is the **sole authoritative** mechanism for ensuring exactly-once processing in the "Hot Path". This is a critical control and must not be compromised.
*   **Optimistic Locking:** The use of a `version` attribute and condition expressions for updating user profiles and configurations is a non-negotiable requirement to prevent lost updates from concurrent operations.
*   **CQRS (Command Query Responsibility Segregation):** The system implicitly uses a CQRS pattern.
    *   **Commands:** Writing a message to the `HotPathSyncQueue` is a "command" to change the system's state.
    *   **Queries:** Reading user configuration or sync status from DynamoDB is a "query".
    *   **Guidance:** This separation is a strength. The team should avoid adding synchronous "read-your-writes" logic to the command path, as this would increase coupling and reduce scalability. Instead, the client should be designed to poll for the result of a command or receive a push notification.
*   **Batching:** The use of DynamoDB's `BatchWriteItem` operation within the worker Lambda is a critical performance and cost optimization. Code reviews should verify that individual `PutItem` calls are not being made inside a loop where a batch operation would be more efficient.

---

## 6. Security & Compliance

The architecture document outlines a strong security posture. This section provides a prioritized list of vulnerabilities and concrete controls required to harden the system for production.

### 6.1. Attack-Surface Summary & Top 10 Vulnerabilities

The primary attack surface consists of:
1.  **The Public API Gateway:** Exposed to the internet, it is the main entry point for attacks against the backend logic.
2.  **Third-Party Webhooks:** An ingress point that can be used for DoS or to inject malicious data if not properly validated.
3.  **The Mobile Client:** Can be reverse-engineered, and vulnerabilities could lead to client-side data leakage or abuse of the backend API.
4.  **Third-Party Dependencies:** Vulnerabilities in open-source packages (NPM, Maven) are a major source of risk.

**Top 10 Prioritized Vulnerabilities & Missing Controls:**

1.  **BLOCKER - Missing Per-User Rate Limiting:** The lack of per-user abuse protection is the single most critical security gap. A single malicious or buggy authenticated user could cause a massive DoS or financial exhaustion attack.
2.  **BLOCKER - Insecure Direct Object Reference (IDOR) Risk:** The architecture relies on the authorizer to control access, but there is no explicit mention of how a worker Lambda is prevented from accessing data belonging to another user *after* it has been authorized.
3.  **High - Single Point of Failure for Auth (Firebase):** As noted previously, the dependency on a single external identity provider is a high-impact availability and security risk.
4.  **High - Risk of PII Leakage in Logs:** The tiered logging strategy is excellent for cost but increases the risk that sensitive data from `PRO` users could be logged. The proposed CloudWatch Logs Data Protection is a good control but needs to be explicitly configured and tested.
5.  **Medium - Insufficient Egress Controls:** The hybrid firewall model is advanced but complex. A misconfiguration could allow a compromised worker to exfiltrate data to an untrusted destination.
6.  **Medium - Lack of Automated Dependency Scanning:** The document mentions Snyk/Dependabot, but this is not optional. An automated dependency scanning tool that fails the build on critical vulnerabilities must be integrated into the CI/CD pipeline.
7.  **Medium - No Formal Secrets Rotation Policy:** The document mentions secret rotation but does not define a formal cadence or an emergency procedure.
8.  **Medium - No Web Application Firewall (WAF) Rule Details:** The architecture specifies AWS WAF but provides no detail on the rules to be configured.
9.  **Low - No Client-Side Hardening Mentioned:** The focus is on the backend, but the mobile client should be hardened against reverse engineering (e.g., via code obfuscation) and tampering.
10. **Low - Lack of Security-Specific Logging & Alerting:** The observability plan is focused on performance and availability. It needs to be augmented with security-specific alerts (e.g., for WAF rule triggers, suspicious IAM activity).

### 6.2. Concrete Remediation & Hardening

#### **Remediation for IDOR (Vulnerability #2)**
The `AuthorizerLambda` must inject the authenticated `userId` into the request context that is passed to the `WorkerLambda`. The `WorkerLambda` **must** then use this context `userId` as the partition key for all DynamoDB queries. It must **never** trust a `userId` provided in a request body.

*   **Example IAM Policy Snippet (Least Privilege):**
    This policy, attached to the `WorkerLambda`, uses IAM policy variables to restrict its access to DynamoDB items that are tagged with the same `userId` as the principal (the Lambda function) that is running. This is a powerful defense-in-depth control.

    ```json
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Allow",
          "Action": [
            "dynamodb:GetItem",
            "dynamodb:PutItem",
            "dynamodb:UpdateItem"
          ],
          "Resource": "arn:aws:dynamodb:us-east-1:123456789012:table/SyncWellMetadata",
          "Condition": {
            "ForAllValues:StringEquals": {
              "dynamodb:LeadingKeys": [
                "USER#${aws:PrincipalTag/userId}"
              ]
            }
          }
        }
      ]
    }
    ```

#### **Remediation for WAF & Rate Limiting (Vulnerabilities #1 & #8)**
*   **AWS WAF Configuration:** The WAF WebACL associated with the API Gateway must have the following rules, in this order:
    1.  **AWS Managed Rule: `AWSManagedRulesCommonRuleSet`:** Protects against the OWASP Top 10.
    2.  **AWS Managed Rule: `AWSManagedRulesAmazonIpReputationList`:** Blocks known malicious IP addresses.
    3.  **Custom Rule: Global Rate-Based Rule:** Block any source IP that sends more than **1,000 requests in a 5-minute period**.
*   **Per-User Rate Limiting:** Implement API Gateway Usage Plans as described in Section 3.

#### **Remediation for Secrets Management (Vulnerability #7)**
*   **Secrets Management:** All secrets must be stored in AWS Secrets Manager, encrypted with a dedicated customer-managed KMS key.
*   **Rotation Policy:**
    *   **Automated Rotation:** Must be enabled for all secrets that support it (e.g., RDS credentials if used in the future). The rotation cadence must be set to **90 days**.
    *   **Manual Rotation:** For other secrets (e.g., third-party API keys), a quarterly manual rotation process must be documented and tracked via recurring tickets assigned to the SRE team.
    *   **Emergency Rotation:** A runbook for out-of-band, emergency rotation must be created and placed in `../ops/runbook-emergency-secret-rotation.md`.

### 6.3. Encryption Strategy

*   **Encryption in Transit:** All public endpoints (API Gateway, CloudFront) must be configured with a security policy that enforces **TLS 1.2 or higher**. All internal traffic between services must also be encrypted.
*   **Encryption at Rest:** All AWS services that store data (DynamoDB, S3, ElastiCache snapshots, SQS queues) must have server-side encryption **enabled**. Where possible, this should use a customer-managed KMS key instead of an AWS-managed key to provide a stronger audit trail.
*   **KMS Key Rotation:** The KMS keys used for at-rest encryption must have **automated annual rotation enabled**.

### 6.4. Compliance Controls

This architecture provides a strong foundation for future compliance audits but does not constitute certification.
*   **GDPR:** The architecture supports key GDPR principles:
    *   **Data Minimization:** The ephemeral processing of health data adheres to this.
    *   **Right to Access/Erasure:** The implementation of the `POST /v1/export-jobs` and `DELETE /v1/users/me` endpoints are necessary controls for this.
*   **SOC 2:** The controls recommended in this review map directly to SOC 2 trust service criteria:
    *   **Security:** IAM policies, WAF, encryption, vulnerability scanning.
    *   **Availability:** Multi-AZ/Multi-Region architecture, DR plans, SLO monitoring.
    *   **Confidentiality:** Encryption at rest and in transit, strict access controls.
    *   **Change Management:** The CI/CD pipeline with automated testing and approval gates is a required control.

---

## 7. Observability & SRE Practices

A robust observability strategy is critical for operating this system reliably and cost-effectively. The tiered and sampled logging approach is a key architectural pattern that must be implemented correctly.

### 7.1. Required Telemetry

*   **Metrics:**
    *   **Standard:** All Lambda functions must use the **CloudWatch Embedded Metric Format (EMF)** to publish custom business and operational metrics (e.g., `SyncSuccessRate`, `CacheHitRate`). This is more cost-effective than making individual `PutMetricData` API calls.
    *   **Cardinality Guidance:** High-cardinality dimensions (like `userId` or `jobId`) **must not** be used in CloudWatch metrics as this will lead to exorbitant costs. Use low-cardinality dimensions like `Tier: [PRO|FREE]`, `Outcome: [SUCCESS|FAILURE]`, `Provider: [Fitbit|Strava]`.
*   **Distributed Tracing:**
    *   **Standard:** **OpenTelemetry** must be adopted as the standard for instrumenting all services.
    *   **Correlation ID:** A `correlationId` (the trace ID) must be generated at the edge (API Gateway) for every request. This ID **must** be propagated through all services (SQS, Lambda) and **must** be included in every log line. This is non-negotiable for enabling distributed tracing.
*   **Structured Logging:**
    *   All log output **must** be structured JSON.
    *   The **AWS Lambda Powertools** library should be used as it provides out-of-the-box support for structured logging, tracing, and metrics.
    *   Every log entry must contain the `correlationId`.

### 7.2. SLI/SLO Examples & Alerting

The SLOs are defined in Section 4. The following are examples of the alerts that must be configured to monitor them.

*   **High-Severity Alert (Pages On-Call Engineer):**
    *   **Name:** `P0 - Core API Availability Below SLO`
    *   **Metric:** `(count(non-5xx responses) / count(total responses)) * 100`
    *   **Threshold:** `Value < 99.95%`
    *   **Evaluation Period:** For 5 consecutive periods of 1 minute.
    *   **Action:** Page the SRE on-call via PagerDuty.

*   **Low-Severity Alert (Creates a Ticket):**
    *   **Name:** `P2 - Cache Latency Nearing SLO`
    *   **Metric:** `p99(ElastiCache_Latency)`
    *   **Threshold:** `Value > 10ms`
    *   **Evaluation Period:** For 30 consecutive periods of 1 minute.
    *   **Action:** Create a `P2-High` ticket in Jira assigned to the core backend team.

### 7.3. Must-Have Dashboards & Runbooks

*   **Required Dashboards:**
    1.  **API Gateway Health:** RPS, 4xx/5xx error rates, p95/p99 latency, per-endpoint breakdown.
    2.  **Lambda Worker Health:** Invocation count, throttles, error rate, average/p99 duration, provisioned vs. on-demand concurrency.
    3.  **Business KPIs:** DAU, `SyncSuccessRate`, new user signups, Pro tier conversion rate, churn rate.
    4.  **Database Health:** Consumed vs. provisioned RCU/WCU, throttle count, latency, GSI utilization.
    5.  **Cost Analysis:** Real-time estimated cost by service (via AWS Cost Explorer), with alerts for anomalies.
    6.  **Security Operations:** WAF blocked requests, API key usage patterns, suspicious IAM activity from CloudTrail.

*   **Required Runbooks:**
    1.  `runbook-region-failover.md` (Created in Section 4)
    2.  `runbook-db-cache-failure.md` (Created in Section 4, expanded to include cache)
    3.  `runbook-traffic-spike.md` (Created in Section 4)
    4.  **`runbook-third-party-outage.md`:** Steps to identify and isolate a failing third-party API, including how to disable a specific provider via AppConfig to prevent cascading failures.
    5.  **`runbook-deployment-rollback.md`:** Step-by-step guide for rolling back a failed canary deployment, including how to analyze canary metrics and how to force a full rollback via the CI/CD system.

### 7.4. Chaos & Load Testing Program

*   **Tools:**
    *   **Chaos Engineering:** **AWS Fault Injection Simulator (FIS)**.
    *   **Load Testing:** **k6** (scripts must be stored in the `/load-testing` directory of the repository).
*   **Scenarios:** The catalog of experiments must include, at a minimum:
    *   AZ failure (terminate all instances in one AZ).
    *   Lambda function failure (inject failure into the Lambda runtime).
    *   API latency injection (add delay to calls to third-party APIs).
    *   Dependency failure (block access to DynamoDB, SQS, or Secrets Manager).
    *   Cache cluster failure (reboot the ElastiCache cluster).
    *   DNS failure (simulate failure to resolve a critical internal or external endpoint).
*   **Schedule:**
    *   **Load Testing:** A suite of k6 tests must be run against the `staging` environment as a required gate in the CI/CD pipeline before any production deployment.
    *   **Chaos Engineering:**
        *   Automated chaos experiments must be run **weekly** in the `staging` environment.
        *   A formal "gameday" where chaos experiments are run manually in the **production** environment must be conducted **quarterly**.

---

## 8. CI/CD, Release Strategy & Runbook Automation

A mature CI/CD process is non-negotiable for deploying changes to this complex system safely and frequently.

### 8.1. Branching, Testing Gates & Release Strategy

*   **Branching Strategy:** A **trunk-based development** model is recommended for services, with short-lived feature branches.
    *   `main`: The trunk. Represents the latest production-ready code.
    *   `feature/<ticket-id>-<description>`: Short-lived branches for new development. Must be merged to `main` via a Pull Request with mandatory reviews and passing checks.
    *   Hotfixes are created from a tag on `main` and merged back in.
*   **Release Strategy (Canary Deployments):** All backend services **must** be deployed to production using a **canary release strategy** managed by AWS CodeDeploy or a similar tool.
    *   **Process:**
        1.  Deploy the new version to a small percentage of the fleet (the "canary"), e.g., **5%**.
        2.  Route **10%** of production traffic to the canary.
        3.  **Bake Time:** Monitor the canary for **30 minutes**.
        4.  **Automated Rollback:** An automatic rollback **must** be triggered if key canary metrics (error rate, latency) deviate significantly from the baseline fleet, as defined in the architecture document.
        5.  **Gradual Rollout:** If the canary is healthy, gradually shift traffic and deploy to the rest of the fleet over a defined period (e.g., 1 hour).
*   **Database Migration Pattern:** For any changes to the DynamoDB schema (e.g., adding a new attribute that code depends on), the **expand/contract pattern** must be used to ensure zero-downtime deployments.
    *   **Release 1 (Expand):** Deploy code that can read both the old and new schema but only writes the old schema. Deploy code to start writing the new schema in addition to the old.
    *   **Release 2 (Migrate):** Run a data migration script to backfill the new schema for all existing items.
    *   **Release 3 (Contract):** Deploy code that now only reads/writes the new schema. Remove the code that handles the old schema.

### 8.2. Required Automated Tests & Pre-Deploy Gates

The following checks **must** be automated in the CI/CD pipeline and must pass before a deployment to production can occur. A failure in any of these gates must fail the build.

1.  **Static Analysis:** Detekt (Kotlin) and SwiftLint (iOS) with project-specific rules.
2.  **Unit Tests:** A minimum of **80% line coverage** must be maintained for all new code.
3.  **Security Scans:**
    *   **SAST (Static Application Security Testing):** Snyk Code or similar.
    *   **Dependency Scanning:** Snyk Open Source or GitHub Dependabot. Must fail the build for any `Critical` or `High` severity vulnerabilities.
4.  **Integration Tests:** A suite of tests that run against a live `staging` environment.
5.  **Contract Tests (Pact):** The backend provider pipeline must run a Pact verification check. A failure here indicates a breaking change and must block the deployment.
6.  **Load Tests (k6):** The k6 performance test suite must be run against the `staging` environment. The deployment is blocked if p99 latency exceeds the SLO or if the error rate is >1%.

### 8.3. Example CI/CD Pipeline Snippet (GitHub Actions)

This snippet illustrates a simplified deployment pipeline for a backend service, showing the key gates and the canary deployment strategy.

```yaml
# .github/workflows/deploy-worker.yml
name: Deploy Worker Service

on:
  push:
    branches:
      - main

jobs:
  test-and-build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Run unit tests
        run: ./gradlew test

      - name: Run Snyk security scan
        uses: snyk/actions/golang@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}

      - name: Build and push container image
        run: |
          # Build, tag, and push Docker image to ECR
          # ...

  deploy-to-prod:
    needs: test-and-build
    runs-on: ubuntu-latest
    strategy:
      matrix:
        # Define the canary and full rollout weights
        rollout: [10, 100]
    steps:
      - name: Deploy to Production (Canary: ${{ matrix.rollout }}%)
        uses: aws-actions/amazon-ecs-deploy-task-definition@v1
        with:
          # ... task definition, cluster, service ...
          wait-for-service-stability: true
          codedeploy-app-name: my-app
          codedeploy-deployment-group-name: my-deployment-group
          codedeploy-deployment-config-name: CodeDeployDefault.LambdaCanary${{ matrix.rollout }}Percent5Minutes

      - name: Monitor bake time
        if: matrix.rollout == 10
        run: sleep 300 # 5 minutes bake time

      - name: Check CloudWatch alarms
        if: matrix.rollout == 10
        # Script to check if any critical alarms have fired for the canary
        # If so, exit 1 to trigger a rollback
        run: ./scripts/check-canary-alarms.sh
```

---

## 9. Costs Model & Financial Alignment

The existing cost model in `66-costs-model.md` is a good first step but makes several assumptions that are misaligned with the production-ready architecture defined in this review. This section provides a reconciled financial model.

### 9.1. Verification of Cost Assumptions

The following table highlights the critical mismatches between the current cost model and the recommended architecture.

| Assumption Area | `66-costs-model.md` Estimate | **Recommended Architecture Reality** | Status & Cause |
| :--- | :--- | :--- | :--- |
| **Network Egress** | ~9.1 TB / month | **~16.4 TB / month** | **BLOCKER.** The model underestimates egress by nearly 2x. This is due to a more realistic assumption about "heavy" payload frequency (10%) and size (500KB). |
| **Observability** | $1,250 / month | **~$3,500+ / month** | **BLOCKER.** The model significantly underestimates CloudWatch costs. At 10M syncs/day, log ingestion alone will be a top cost driver. The tiered logging strategy is mandatory to control this. |
| **Security Services**| ~$137 / month (WAF only) | **~$1,000+ / month** | **BLOCKER.** The model omits the cost of AWS Network Firewall and AWS Inspector, both of which have usage-based costs that will be significant at scale. |
| **Database** | $684 / month | **~$1,100 / month** | **BLOCKER.** The model underestimates write costs. It correctly identifies Global Tables double write costs but uses a lower traffic estimate. |
| **Identity DR** | Not costed. | **~$200 / month** | **GAP.** The cost of the recommended Cognito User Pool for DR is missing from the model. |
| **Total Monthly Cost**| **~$7,914 / month** | **~$13,000+ / month (On-Demand)** | **BLOCKER.** The current model understates the true on-demand cost by **at least 65%**. |

### 9.2. Reconciled, Production-Ready Monthly Cost Table

This table presents a more realistic, bottom-up cost estimate based on the **recommended production architecture** and the **Nominal traffic model** (1M DAU, 10M syncs/day, ~16.4TB egress).

| Component | Service | Unit Cost | Units (Monthly) | **Reconciled Cost** | Notes |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Compute** | AWS Lambda (Graviton) | `$0.00001333/GB-s` | 300M invocations, 1024MB, 1.5s duration | **$6,120** | Higher invocation count based on 10 syncs/day. |
| | | `$0.20/M invokes` | 300M invocations | **$60** | |
| **Database** | DynamoDB (On-Demand) | `$1.25/M WCU` | 300M syncs * 2 (Global Table) = 600M WCUs | **$750** | |
| | | `$0.25/M RCU` | 300M syncs * 1 = 300M RCUs | **$75** | |
| | **AWS Backup** | `$0.17/GB-month`| Est. 1 TB table size | **$170** | For managing snapshots. |
| **Caching** | ElastiCache for Redis | `$0.266/hr` | 2x `cache.m6g.large` nodes | **$383** | Aligned with architecture doc. |
| **Network/Egress**| **NAT Gateway** | `$0.045/GB` | **16,400 GB** processed | **$738** | **Critical:** Based on the revised 16.4TB egress model. |
| | **Network Firewall** | `$0.065/GB` | Est. 10% of traffic (1.64TB) | **$107** | Cost for traffic to untrusted endpoints. |
| **Messaging** | API Gateway | `$1.00/M requests` | 300M requests | **$300** | |
| | Amazon SQS (Standard) | `$0.40/M requests` | ~600M requests | **$240** | |
| **Observability**| **CloudWatch Logs** | `$0.50/GB` | Est. 5 TB ingested | **$2,500** | **Critical:** Assumes tiered logging is NOT implemented. |
| | **CloudWatch Metrics**| (Varies) | - | **$750** | High volume of custom metrics. |
| | **CloudWatch Alarms** | (Varies) | - | **$250** | |
| **Security** | AWS WAF | `$0.60/M requests` | 300M requests | **$180** | |
| | AWS Inspector | `$1.25/instance` | Est. 50 Fargate tasks, etc. | **$63** | |
| **Other** | (Secrets Manager, Cognito, etc.) | - | - | **$250** | Includes DR identity provider. |
| **TOTAL** | | | | **~$12,936 / month** | *(On-Demand)* |

### 9.3. Reconciled Cost Metrics & Sensitivity Analysis

*   **Reconciled Cost-per-DAU (Monthly):** `$12,936 / 1,000,000 =` **$0.013 per user per month**.
*   **Reconciled Cost-per-1M-DAU (Annualized):** `$12,936 * 12 =` **$155,232 per year**.

The table below shows how this reconciled cost shifts under different traffic scenarios and with the application of a **3-year, all-upfront Compute Savings Plan (~40% discount)**.

| Traffic Scenario | Reconciled Monthly Cost (On-Demand) | Reconciled Monthly Cost (with 3-Yr Savings Plan) |
| :--- | :--- | :--- |
| **Conservative** | **~$7,000** | **~$4,800** |
| **Nominal** | **~$12,936** | **~$10,200** |
| **Aggressive** | **~$25,000** | **~$21,500** |

**Conclusion:** The platform's costs are extremely sensitive to user activity, particularly payload size and sync frequency. A Savings Plan is a powerful lever, but it cannot fix a fundamentally expensive architecture. The **only** way to control costs is to aggressively implement the application-level optimizations.

### 9.4. Recommended Cost-Optimization Levers (Prioritized)

1.  **MANDATORY - Implement Tiered/Sampled Logging:** This is the highest-impact lever. Failing to implement the tiered logging strategy will result in CloudWatch costs exceeding **$5,000/month**. **Estimated Savings: High ($2,000+/month).**
2.  **MANDATORY - Implement Metadata-First Hydration:** This is the second most critical lever. It directly attacks the largest variable cost driver: network egress. Every 10% reduction in egress saves **~$75/month**. **Estimated Savings: High ($500+/month).**
3.  **Purchase Savings Plans:** Once traffic patterns are predictable (post-beta), purchase a 3-year Compute Savings Plan covering at least 50% of baseline Lambda usage. **Estimated Savings: High ($2,500+/month).**
4.  **Enforce VPC Endpoints:** This is a simple, high-impact change that reduces NAT gateway data processing charges. **Estimated Savings: Medium ($200+/month).**
5.  **Right-size Lambda Memory:** Use AWS Lambda Power Tuning to find the optimal memory configuration for the worker Lambda. Over-provisioning by just 256MB can cost hundreds of dollars per month at this scale. **Estimated Savings: Medium ($100-500/month).**

---

## 10. Cost Model & Optimizations

This section summarizes the reconciled cost model and provides actionable optimization levers.

### 10.1. T-Shirt Cost Estimate (Reconciled, On-Demand)

Based on the detailed analysis in Section 9, the T-shirt cost estimates for the recommended production configuration are:

*   **Low (Conservative Traffic):** **~$7,000 / month**
*   **Mid (Nominal Traffic):** **~$13,000 / month**
*   **High (Aggressive Traffic):** **~$25,000 / month**

**Cost Breakdown (Nominal Scenario):**
*   **Compute (Lambda):** ~48%
*   **Observability (CloudWatch):** ~27%
*   **Database (DynamoDB):** ~8%
*   **Network & Egress:** ~7%
*   **Other (Cache, Messaging, Security, etc.):** ~10%

**Conclusion:** Compute and Observability are the dominant cost drivers, accounting for ~75% of the total monthly spend.

### 10.2. Cost Optimization Levers

The following levers are essential for managing costs. They are categorized by implementation timeline.

#### **Quick Wins (Application & Infrastructure Logic)**
These are the highest priority and should be implemented before public launch.

*   **Tiered & Sampled Logging:** **(High Impact)** This is primarily an application logic change and is the most effective way to control observability costs.
*   **Metadata-First Data Hydration:** **(High Impact)** This application logic change is mandatory to control network egress costs.
*   **Enforce VPC Endpoints:** **(Medium Impact)** This is a straightforward infrastructure change that provides immediate cost savings on NAT Gateway data transfer.

#### **Medium-Term Levers (Commercial & Tuning)**
These should be pursued post-beta, once traffic patterns are better understood.

*   **Purchase Savings Plans:** **(High Impact)** The single most effective way to reduce compute costs. A 3-year plan offers the best discount. This should be purchased for a baseline of observed usage after 1-2 months of stable traffic.
*   **Right-size All Resources:** **(Medium Impact)** Continuously use tools like AWS Lambda Power Tuning and Cost Explorer Rightsizing Recommendations to eliminate waste.
*   **S3 Intelligent-Tiering:** **(Low Impact)** Configure this for log archive buckets to automatically optimize storage costs.

### 10.3. Cost Allocation, Tagging & Reporting

*   **Tagging Policy:** The cost allocation tagging strategy proposed in `66-costs-model.md` is excellent and is adopted as a **mandatory policy**. All cloud resources **must** be tagged with:
    *   `sw:cost-center`
    *   `sw:environment`
    *   `sw:service-name`
    *   `sw:feature`
    *   `sw:owner`
*   **Enforcement:** This policy must be enforced automatically using an **AWS Service Control Policy (SCP)** applied at the root of the AWS Organization.
*   **Reporting:** The finance lead and engineering manager are jointly responsible for creating and reviewing a monthly cost report in AWS Cost Explorer, grouped by the `sw:service-name` and `sw:feature` tags to track spend against budget.

---

## 11. Concrete Artifacts

This section contains the concrete, ready-to-use artifacts generated as part of this review.

### 11.1. Annotated Suggestions for Source Documents

#### **`06-technical-architecture.md` (Suggested Changes)**

1.  **Section 4: Technology Stack - Authentication Service**
    > **Original:** While Amazon Cognito is a native AWS service, Firebase Authentication has been chosen for the MVP due to its superior developer experience... This choice prioritizes rapid development...
    >
    > **Replacement:** While Amazon Cognito is a native AWS service, Firebase Authentication has been chosen for the MVP due to its superior developer experience. **However, this creates a critical single-vendor dependency.** To mitigate this, a **dual-authorizer strategy must be implemented.** The API Gateway Authorizer will be configured to validate JWTs from both Firebase and a standby Amazon Cognito User Pool. A runbook and configuration flag must exist to failover to Cognito in a DR scenario.

2.  **Section 6: Security, Privacy, and Compliance - Security Measures**
    > **Original:** To mitigate the risk of abuse... the architecture will leverage **AWS WAF**. In addition to protecting against common exploits, a **rate-based rule** will be configured to automatically block IP addresses...
    >
    > **Replacement:** To mitigate the risk of abuse, a multi-layered strategy is required. The architecture will leverage **AWS WAF** with a global rate-based rule (e.g., 1,000 requests/IP/5min). Critically, this is insufficient for targeted attacks. **Per-user rate limiting must be implemented** using **API Gateway Usage Plans** tied to individual user API keys to prevent abuse from a single authenticated user.

#### **`66-costs-model.md` (Suggested Changes)**

1.  **Section 3: Reconciled Production Monthly Cost Estimate**
    > **Original:** The entire cost table, which estimates a total of **~$7,914 / month**.
    >
    > **Replacement:** The cost table must be replaced with the **reconciled table from the Production Readiness Review (Section 9.2)**. The new table estimates a total on-demand cost of **~$12,936 / month** and correctly identifies Observability and Network Egress as primary cost drivers, flagging the original estimate as a **BLOCKER** due to a >65% understatement of the true cost.

### 11.2. Prioritized Implementation Roadmap

| Priority | Task | Effort | Owner |
| :--- | :--- | :--- | :--- |
| **Must** | **BLOCKER:** Launch private beta to validate traffic & payload assumptions. | M | Product |
| **Must** | **BLOCKER:** Implement Per-User Rate Limiting (API Gateway Usage Plans). | M | Backend |
| **Must** | **BLOCKER:** Implement application-level Tiered & Sampled Logging. | L | Backend |
| **Must** | **BLOCKER:** Implement Metadata-First Data Hydration pattern. | M | Backend |
| **Must** | Formalize CI/CD pipeline with all required testing gates (Security, Load). | L | SRE/DevOps|
| **Must** | Implement least-privilege IAM policies with IDOR protection. | M | Security |
| **Should** | Implement dual-authorizer (Cognito DR) strategy. | L | Backend |
| **Should** | Establish formal Chaos Engineering program and run first gameday. | M | SRE |
| **Should** | Purchase Compute Savings Plan (after beta validates usage). | S | Finance |
| **Should** | Create all required Dashboards and Runbooks. | M | SRE |
| **Can** | Implement client-side hardening (code obfuscation). | M | Mobile |
| **Can** | Conduct full-table restore drill from AWS Backup. | S | SRE |

### 11.3. IaC Snippet: Autoscaled Lambda Worker (Terraform)

This snippet defines the core `WorkerLambda`, including configuration for ARM64 architecture and Provisioned Concurrency for performance, with variants for cost optimization.

```terraform
# terraform/lambda.tf

resource "aws_lambda_function" "worker_lambda" {
  function_name = "syncwell-prod-worker"
  role          = aws_iam_role.worker_lambda_role.arn
  package_type  = "Image"
  image_uri     = "123456789012.dkr.ecr.us-east-1.amazonaws.com/syncwell-worker:latest"

  # Use ARM64 for better price-performance
  architectures = ["arm64"]
  memory_size   = 1024 # Tuned via Lambda Power Tuning
  timeout       = 300  # 5 minutes

  environment {
    variables = {
      # ... env vars ...
    }
  }

  # Cost Optimization: To eliminate cold starts for performance, uncomment below.
  # This adds a fixed cost but guarantees low latency.
  # provisioned_concurrency_config {
  #   provisioned_concurrent_executions = 50
  # }
}

resource "aws_lambda_provisioned_concurrency_config" "worker_concurrency" {
  count = var.enable_provisioned_concurrency ? 1 : 0

  function_name                     = aws_lambda_function.worker_lambda.function_name
  provisioned_concurrent_executions = 50
  qualifier                         = aws_lambda_function.worker_lambda.version
}

variable "enable_provisioned_concurrency" {
  description = "Set to true to enable provisioned concurrency for the worker Lambda."
  type        = bool
  default     = false # Default to on-demand for cost savings
}
```

### 11.4. Mermaid Diagram Snippet (Recommended Critical Path)

```mermaid
---
title: "Recommended Critical Path (Multi-AZ & Multi-Region)"
---
graph TD
    subgraph "Global / DNS"
        User("`fa:fa-user User`")
        Route53("`fa:fa-route Route 53`")
    end

    subgraph "AWS Region: us-east-1 (Primary)"
        style `AWS Region: us-east-1 (Primary)` fill:#e6f3ff,stroke:#0073bb
        subgraph "Edge"
            CloudFront("`fa:fa-globe-americas CloudFront`")
            WAF("`fa:fa-shield-alt WAF`")
        end
        subgraph "API Layer"
            ApiGateway("`fa:fa-server API Gateway`")
            Authorizer("`fa:fa-bolt Authorizer Lambda`")
            UsagePlan("`fa:fa-tachometer-alt Usage Plan<br>(Per-User Rate Limit)`")
        end
        subgraph "Availability Zone 1"
            style `Availability Zone 1` fill:#ffffff,stroke-dasharray: 5 5
            Lambda_A("`fa:fa-bolt Worker Lambda`")
            Redis_A("`fa:fa-memory Redis Primary`")
        end
        subgraph "Availability Zone 2"
            style `Availability Zone 2` fill:#ffffff,stroke-dasharray: 5 5
            Lambda_B("`fa:fa-bolt Worker Lambda`")
            Redis_B("`fa:fa-memory Redis Replica`")
        end
        SQS("`fa:fa-inbox SQS Queue`")
        Dynamo_P["`fa:fa-database DynamoDB<br>(Primary)`"]
    end

    subgraph "AWS Region: us-west-2 (DR)"
        style `AWS Region: us-west-2 (DR)` fill:#fde4d8,stroke:#d95300
        Dynamo_DR["`fa:fa-database DynamoDB<br>(Replica)`"]
    end

    User --> Route53 --> CloudFront --> WAF --> ApiGateway
    ApiGateway -- "Authorizes via" --> Authorizer
    ApiGateway -- "Enforces" --> UsagePlan
    ApiGateway -- "Enqueues job" --> SQS
    SQS -- "Triggers" --> Lambda_A & Lambda_B
    Lambda_A & Lambda_B -- "Read/Write" --> Redis_A
    Redis_A -- "Replicates to" --> Redis_B
    Lambda_A & Lambda_B -- "Read/Write State" --> Dynamo_P
    Dynamo_P -- "Global Table Replication" --> Dynamo_DR
```

### 11.5. Sample Runbook: DB Failover

This is a copy of the runbook from Section 4 for easy access.

**Runbook: Master Database Failure (DynamoDB Unavailability in Primary AZ)**
*Note: With DynamoDB, a "master" failure is not the right model. This runbook addresses the unavailability of the DynamoDB endpoint in a single AZ or the entire primary region.*

1.  **Acknowledge Alert:** Acknowledge the CloudWatch alarm for "High DynamoDB Read/Write Errors" or "High DynamoDB Latency".
2.  **Assess Impact:**
    *   Check if the errors are isolated to a single AZ. If so, the multi-AZ Lambda deployment should handle this gracefully. Monitor the error rate to ensure it does not escalate.
    *   Check if the errors are region-wide. This indicates a larger-scale problem.
3.  **Verify Application Behavior:**
    *   Check if sync jobs are being correctly retried and eventually sent to the DLQ.
    *   Confirm that the application is failing "open" or "closed" as expected (e.g., returning a 503 error to clients).
4.  **Trigger Manual Failover (if region-wide):**
    *   If DynamoDB is unavailable across the entire `us-east-1` region, the incident becomes a **Region Outage**.
    *   **Immediately escalate to the "Primary Region Outage" runbook.** The procedure is the same: fail over DNS and data plane to the `us-west-2` DR site.
5.  **Communicate:** Keep the status page and internal stakeholders updated on the impact and mitigation steps.

### 11.6. Production Readiness Gate Checklist

This checklist must be completed and signed off by the CTO, Head of Engineering, and Finance Lead before public launch.

| Category | Item | Status | Sign-off |
| :--- | :--- | :--- | :--- |
| **Architecture** | Final architecture diagram reflects implemented reality. | [ ] Done | CTO |
| | All "Must-Have" tasks from the roadmap are complete. | [ ] Done | Eng Lead |
| | Private beta has run and traffic assumptions are validated/updated. | [ ] Done | Product |
| **Availability** | Multi-AZ and Multi-Region DR strategy is fully implemented and tested. | [ ] Done | SRE |
| | All required SLOs, dashboards, and alerts are configured. | [ ] Done | SRE |
| | A successful chaos engineering gameday has been completed in staging. | [ ] Done | SRE |
| **Security** | All BLOCKER vulnerabilities from review are remediated. | [ ] Done | Security |
| | WAF, per-user rate limiting, and least-privilege IAM are implemented. | [ ] Done | Security |
| | A clean Snyk/dependency scan report is available for the release candidate. | [ ] Done | Eng Lead |
| **Operations** | CI/CD pipeline is fully automated with all required gates. | [ ] Done | SRE |
| | All required runbooks are created and accessible to on-call staff. | [ ] Done | SRE |
| | On-call rotation is established and PagerDuty is configured. | [ ] Done | Eng Manager |
| **Cost** | **Reconciled cost model is reviewed and accepted.** | [ ] Done | **Finance** |
| | **Monthly budget is agreed upon and a budget owner is assigned.** | [ ] Done | **Finance** |
| | Cost allocation tagging policy is implemented and enforced. | [ ] Done | SRE |
| | Application-level cost optimizations (logging, hydration) are implemented. | [ ] Done | Eng Lead |

---

## 12. Risks, Open Questions & Blockers

This final section summarizes the most critical risks and open questions that must be addressed before proceeding with a public launch.

### 12.1. Top 10 Risks (Likelihood  Impact)

1.  **Financial Overrun (High  High):** The cost model is extremely sensitive to unvalidated traffic and payload assumptions. A 2x increase in "heavy" syncs could increase monthly costs by thousands.
2.  **Third-Party Outage (Low  High):** A major, prolonged outage of Firebase Authentication would render the entire application unusable. The Cognito DR plan is a mitigation, not a preventative measure.
3.  **Economic Denial of Service (Medium  High):** The lack of per-user rate limiting exposes the platform to financial exhaustion attacks from a single malicious or buggy authenticated client.
4.  **Execution Complexity (High  Medium):** The architecture relies on sophisticated, custom application-level logic (tiered logging, metadata hydration). Failure to implement and test these patterns correctly will lead to significant cost and performance issues.
5.  **Data Inconsistency (Medium  Medium):** The "newest wins" conflict resolution strategy is simple but may lead to data loss scenarios that frustrate users.
6.  **Vendor Lock-in (High  Medium):** The architecture is deeply tied to proprietary AWS services. This is an accepted strategic risk, but it makes future platform migrations extremely costly.
7.  **Compliance/Data Breach (Low  High):** A security breach resulting in the leak of user health data would have severe legal, financial, and reputational consequences.
8.  **Performance Degradation (High  Low):** While provisioned concurrency helps, unexpected traffic spikes could still lead to cold starts and a degraded user experience for a subset of users.
9.  **Alert Fatigue (High  Medium):** A poorly tuned monitoring system can generate excessive noise, causing on-call engineers to ignore critical alerts.
10. **Scalability Bottleneck (Medium  Medium):** The "hot partition" risk for a viral user, while deferred, could cause significant performance degradation for that user and their immediate neighbors in the data store.

### 12.2. Open Questions & Blockers

The following questions must be answered before this project proceeds to a full public launch. They are considered **BLOCKERS** because they fundamentally impact the project's capacity, cost, and legal standing.

*   **BLOCKER 1: What are the real-world traffic and payload metrics from a private beta?**
    *   **Why it blocks:** All capacity planning and financial forecasting in this review are based on a series of nested assumptions. The financial risk of launching without validating these assumptions is unacceptably high. The team cannot make an informed budget decision without this data.

*   **BLOCKER 2: Who is the assigned budget owner, and has the reconciled monthly cost model (~$13k/mo on-demand) been approved?**
    *   **Why it blocks:** The reconciled cost is significantly higher than all previous estimates. Proceeding without formal financial approval and a named owner for the budget is fiscally irresponsible and puts the project at risk of being defunded post-launch.

*   **BLOCKER 3: What is the specific, documented, and tested disaster recovery plan for a multi-hour Firebase Authentication outage?**
    *   **Why it blocks:** The current mitigation (a dual authorizer) is a technical prerequisite but not a complete DR plan. A formal plan, including client-side failover logic, user communication strategy, and an estimated RTO, must be documented and approved by leadership.

*   **BLOCKER 4: What are the specific data residency requirements for all target launch markets?**
    *   **Why it blocks:** The current architecture stores all data in `us-east-1`. If the company plans to launch in markets with strict data residency laws (e.g., the EU/GDPR), this architecture may not be compliant. Legal and compliance teams must provide a definitive answer to this question before launch to avoid significant legal and reputational risk.
