# Technical Architecture Review: f-17.md

---

## 1. Summary of Findings

### Executive Summary
The technical documentation for the SyncWell application outlines a modern, scalable, and ambitious cloud-native architecture. The overall maturity of the design is high, demonstrating a strong grasp of serverless principles, high-availability patterns, and security best practices. The use of C4 modeling, detailed data flow descriptions, and comprehensive non-functional requirements provides a solid foundation for development.

However, the documentation suite, in its current state, suffers from several **critical contradictions, significant ambiguities, and unaddressed operational risks** that could jeopardize the project's timeline, budget, and long-term viability. While many individual components are well-defined, the connections between them are not always clear, and several key operational procedures are either overly manual or not fully specified.

### Most Critical Findings
The following issues require immediate attention before any further implementation proceeds:

1.  **Contradictory Distributed Lock Implementation:** The architecture specifies two different, mutually exclusive implementations for distributed locking: **ElastiCache for Redis** (`06-technical-architecture.md`) and **DynamoDB Conditional Writes** (`18-backup-recovery.md`). This is a critical flaw that must be resolved, as an incorrect implementation in a multi-region setup can lead to severe data corruption.
2.  **Drastic Discrepancy in Scalability Projections:** The required Lambda concurrency to meet the 3,000 RPS target is calculated as **15,000** in `06-technical-architecture.md` and **45,000** in `16-performance-optimization.md`. This 3x difference has catastrophic implications for cost modeling and technical feasibility. The higher, more realistic number suggests the project may be financially and operationally unviable without significant architectural changes.
3.  **Ambiguous Runtime for Critical Components:** The architecture mandates a fast-starting runtime (TypeScript/Python) for the `AuthorizerLambda` to meet latency SLOs, yet also states the backend will leverage a KMP/JVM module (`06-technical-architecture.md`). This fundamental contradiction in the technology stack for a security-critical, latency-sensitive component is a major flaw.
4.  **Insufficiently Defined "Break-Glass" and "Hot User" Procedures:** The manual, PR-based "break-glass" procedure for debugging user issues and the manual "hot user" migration process are significant operational risks. They introduce unacceptable latency into critical support and scaling workflows and are prone to human error. These processes are not scalable for a service targeting 1M DAU.
5.  **Data Privacy Violation in Logging Strategy:** The `17-error-handling.md` document states that `userId` **must not** be logged to protect privacy. However, the `19-security-privacy.md` document defines a `SyncWellBreakGlassIndex` table whose entire purpose is to log the `userId` alongside a `correlationId`, directly contradicting the stated logging policy.

---

## 2. Logical Flaws, Bugs, and Inconsistencies

| ID | Description | Impact/Severity | Recommendation/Question |
|:---|:---|:---|:---|
| **LFI-001** | **Contradictory Distributed Lock Implementation:** `06-technical-architecture.md` states ElastiCache for Redis will be used for distributed locking. `18-backup-recovery.md` correctly identifies this as an anti-pattern for multi-region active-active setups and specifies using DynamoDB conditional writes. | **Critical** | This is a fundamental architectural contradiction. The DynamoDB approach is correct for multi-region consistency. The entire documentation suite must be updated to reflect this, and all diagrams showing Redis for locking must be corrected. |
| **LFI-002** | **Contradictory Lambda Concurrency Calculation:** The peak Lambda concurrency is calculated as 15,000 in `06-technical-architecture.md` but 45,000 in `16-performance-optimization.md`. | **Critical** | The 45,000 figure (based on P90 duration) is more realistic for SLO planning. This discrepancy invalidates the cost and risk assessment in the main architecture document. All documents must be reconciled to use the higher, worst-case figure. |
| **LFI-003** | **Contradictory `AuthorizerLambda` Runtime:** `06-technical-architecture.md` (Section 4) states the `AuthorizerLambda` **must** be written in TypeScript or Python for performance, but also states the backend will use a KMP/JVM module for code reuse. | **Critical** | This is a direct contradiction regarding a critical component. Clarify the definitive technology choice. Is the authorizer an exception to the KMP strategy? If so, this needs to be explicitly stated. |
| **LFI-004** | **Contradictory Logging Policy:** `17-error-handling.md` states "permanent identifiers like `userId` **must not** be logged." `19-security-privacy.md` defines the `SyncWellBreakGlassIndex` which explicitly logs `userId` and `correlationId`. | **High** | This is a direct contradiction on a key privacy issue. The existence of the break-glass index must be acknowledged in the logging strategy, and the privacy implications must be clarified. The statement "must not be logged" is factually incorrect if the index exists. |
| **LFI-005** | **Idempotency Diagram Inaccuracy:** The sequence diagram for idempotency (`06-technical-architecture.md`, Section 3a) shows the worker deleting the idempotency key from Redis on failure. This is incorrect for SQS-based retries. If the key is deleted, a redriven message would be processed again, defeating the purpose of "at-most-once" processing for a completed job. | **High** | The key for a *successful* job should be retained for the full TTL. The diagram and logic should be corrected to only delete the "INPROGRESS" lock on failure, allowing a clean retry. The final "COMPLETED" key must not be deleted. |
| **LFI-006** | **Inconsistent Idempotency Store:** The idempotency strategy in `06-technical-architecture.md` clearly states Redis is the store. However, the distributed lock strategy in `18-backup-recovery.md` uses DynamoDB. These are related concepts; could a single DynamoDB implementation serve both purposes to reduce complexity? | **Medium** | The use of two separate systems (Redis for idempotency, DynamoDB for locking) for closely related semantic purposes adds complexity. Assess if a single, well-designed DynamoDB table could handle both idempotency and distributed locking. |
| **LFI-007** | **SQS DLQ Logic Flaw:** The "State Machine for a Sync Job" diagram in `05-data-sync.md` shows a state transition from `Failed` to `Moved to DLQ`. This is misleading. SQS moves the message to the DLQ *after* the `maxReceiveCount` is exceeded; a single failure does not trigger this. | **Medium** | The diagram is conceptually incorrect. It should show a path from "Retrying" to "Moved to DLQ" after N retries. This clarifies that failure is not an immediate terminal state. |
| **LFI-008** | **Contradictory Cache Location for JWT Keys:** `06-technical-architecture.md` (Section 3b) lists two different caches for JWT Public Keys: "in API Gateway" and "in-memory inside the authorizer Lambda". These are two distinct caching layers. | **Low** | The documentation should clarify that there are two caching layers in play: API Gateway's cache (L1) and an in-memory cache within the Lambda (L2) for when the L1 cache misses. The current wording is slightly ambiguous. |
| **LFI-009** | **Incorrect `SYNCCONFIG` SK format:** `06-technical-architecture.md` (Section 3c) defines the Sort Key for a Sync Config as `SYNCCONFIG#{sourceId}#{destId}#{dataType}`. However, `05-data-sync.md` defines it as `SYNCCONFIG#{sourceId}#to#{destId}#{dataType}`. | **Low** | This is a minor but important inconsistency in the data model. A single, definitive format must be chosen and used consistently across all documents. The version with "to" is more readable. |
| **LFI-010** | **Contradictory ElastiCache Replication Strategy:** `18-backup-recovery.md` states "each regional ElastiCache cluster will operate independently" and a failover will result in a cold cache. However, `06-technical-architecture.md` mentions "ElastiCache Global Datastore" in the cost drivers section, which implies cross-region replication. | **High** | This is a major contradiction. Is the cache replicated or not? The "independent cluster" approach described in the recovery document is the correct pattern for an active-active architecture to avoid cache consistency issues. The reference to Global Datastore should be removed if it's not being used. |
| **LFI-011** | **Historical Sync Trigger Inconsistency:** `06-technical-architecture.md` says API Gateway starts the Step Functions execution for historical syncs. `05-data-sync.md` says the same. However, the API contract in `06-technical-architecture.md` (Section 3d) for `POST /v1/sync-jobs` has no field to specify a historical sync. | **Medium** | The API contract is missing a way to trigger a historical sync. Should there be a `mode: "historical"` in the request body, or a separate endpoint like `/v1/historical-sync-jobs`? This is a gap. |
| **LFI-012** | **"Last Writer Wins" Conflict:** `06-technical-architecture.md` states that for DynamoDB Global Tables, the application will rely on the default "last writer wins" conflict resolution. This is a high-risk strategy for user configuration data, as it can lead to silent data loss if a user makes changes in two regions close together. | **High** | Relying on "last writer wins" is a dangerous default. A more robust strategy, such as using version numbers and conditional writes to detect and reject conflicting updates, should be implemented for critical user settings. |
| **LFI-013** | **Incorrect `PushResult` Usage:** The `DataProvider` interface in `07-apis-integration.md` defines `pushData` returning a `PushResult` with `failedItemIds`. However, the sync algorithm in `05-data-sync.md` does not mention checking this result or handling partial failures. It seems to assume the entire write succeeds or fails. | **Medium** | The system needs a defined strategy for handling partial write failures. What happens if 5 out of 10 records fail to write? Is the whole job retried? Is the `lastSyncTime` updated? This is a gap in the logic. |
| **LFI-014** | **Contradictory `ProviderTokens` Model:** The `CanonicalData` sealed interface is defined in `06-technical-architecture.md`, but the `CanonicalWorkout` class does not inherit from it. The `ProviderTokens` class in `06-technical-architecture.md` is defined twice with different fields. | **Medium** | The code examples have logical errors. The `CanonicalWorkout` class should implement `CanonicalData`. The `ProviderTokens` model needs a single, definitive definition. This points to a lack of careful review of the code snippets. |
| **LFI-015** | **Mismatched Mermaid Diagram and Text:** In `06-technical-architecture.md` (Section 3a), the idempotency sequence diagram shows the worker making a `PUT` with `State: INPROGRESS` and `State: COMPLETED`. However, the text only mentions storing the final JSON response. The diagram implies a state machine that is not described in the text. | **Medium** | The diagram and the text describing the idempotency flow are inconsistent. The diagram's state-based approach (`INPROGRESS`, `COMPLETED`) is more robust. The text should be updated to match this more detailed implementation. |
| **LFI-016** | **Automatic Syncs not in API Contract:** `06-technical-architecture.md` (Section 3d) notes that automatic syncs are not triggered via the public API. However, the `POST /v1/sync-jobs` request body includes `"mode": "manual"`, which implies other modes exist that could be sent. | **Low** | This is ambiguous. If only manual syncs can be triggered via this endpoint, the `mode` field should be removed or the API documentation should explicitly state that other values will be rejected. |
| **LFI-017** | **Inconsistent Naming for AI Service:** The future AI service is referred to as "AI Insights Service" and "AI-Powered Merge" service interchangeably. In one diagram, it is `AI_Service`. | **Low** | Terminology should be consistent. A single, definitive name should be chosen and used throughout all documents and diagrams. |
| **LFI-018** | **Inconsistent Hot/Cold Path Naming:** The terms "Hot Path"/"Cold Path" are used extensively but are not consistently capitalized or hyphenated. | **Low** | For professional documentation, consistent terminology is important. Standardize on "hot path" and "cold path". |
| **LFI-019** | **Contradiction on `isHot` Flag Location:** `06-technical-architecture.md` states the `isHot` flag is on the `PROFILE` item. A later section says the data access layer reads the `PROFILE` item *from the main table* to check the flag. If the user's profile has been migrated, how can it be read from the main table? | **High** | This is a logical flaw in the hot user migration process. The "pointer" to the hot user must remain in the main table. The document should clarify that a "pointer" or "shadow profile" remains in the main table, containing only the `isHot` flag and the user ID. |
| **LFI-020** | **Rate Limit Prioritization Logic:** `07-apis-integration.md` states the rate limiter will prioritize hot-path jobs over cold-path jobs. This is good, but the mechanism for how the rate limiter *knows* the job's priority is not defined. | **Medium** | How is the priority passed to the rate limiting service? The job event payload must include a `priority` field (`hot` or `cold`) that the worker can use when requesting a token from the rate limiter. This is a missing detail. |
| **LFI-021** | **Incorrect Error Code in Diagram:** The idempotency diagram in `06-technical-architecture.md` shows `ConditionalCheckFailedException` being returned from Redis. This is a DynamoDB exception. The equivalent Redis error would be a `nil` reply from a Lua script. | **Low** | This is a technical inaccuracy in the diagram that shows a misunderstanding of the chosen technology. The diagram should be corrected to reflect the correct Redis behavior. |
| **LFI-022** | **Contradictory `DataProvider` Interface:** The `DataProvider` interface is defined in `06-technical-architecture.md` and again in `07-apis-integration.md`. The version in `07-apis-integration.md` is more complete, including the `capabilities` field. | **High** | There must be a single, definitive source of truth for this critical interface. The more complete version from `07-apis-integration.md` should be considered canonical, and the other document should reference it. |
| **LFI-023** | **Missing `jobId` in Sync Job Request:** The `POST /v1/sync-jobs` response contains a `jobId`, but the request does not. How is this `jobId` generated? Is it the same as the `Idempotency-Key`? | **Medium** | Clarify the relationship between `jobId` and `Idempotency-Key`. If they are the same, the response should probably label it `idempotencyKey`. If not, the source of the `jobId` needs to be defined. |
| **LFI-024** | **Unrealistic RTO for Cache Failure:** `18-backup-recovery.md` lists an RTO of "< 60 minutes" for a manual promotion of a secondary ElastiCache replica. This seems excessively long for a critical component. | **High** | An hour of degraded performance and disabled features due to a cache failure is a major incident. The document should state that ElastiCache Multi-AZ provides an *automatic* failover with an RTO of < 1 minute. The manual promotion runbook should only be for a full *Global Datastore* regional failure, not a simple node failure. |
| **LFI-025** | **Inconsistent `ProviderTokens` Definition:** The `ProviderTokens` data class is defined at the end of `06-technical-architecture.md` with `issuedAtEpochSeconds`. A different, incomplete definition appears earlier. | **Medium** | There are multiple, conflicting definitions of the same data model in a single document. This must be consolidated into a single, canonical definition. |

---

## 3. Clarity, Completeness, and Ambiguity

| ID | Description | Impact/Severity | Recommendation/Question |
|:---|:---|:---|:---|
| **CCA-001** | **Missing Diagrams:** Many sections in the documents refer to diagrams that are marked as `[Placeholder - Diagram to be created]`. | **High** | The documents are incomplete without these visuals. Key missing diagrams include the caching architecture and the job chunking flow. These must be created to ensure the concepts are clearly understood. |
| **CCA-002** | **Undefined "Break-Glass" Alerting:** The "break-glass" procedure in `19-security-privacy.md` is detailed, but it doesn't specify how the second engineer is notified that a PR is ready for their review. For a time-sensitive issue, relying on email or Slack DMs is not a robust process. | **High** | A formal, automated notification mechanism should be part of the process. Should this approval request be posted to a specific, monitored Slack channel or trigger a PagerDuty alert for the secondary on-call? |
| **CCA-003** | **Ambiguous Google Fit Integration:** `06-technical-architecture.md` describes the Google Fit integration as "Hybrid (Device & Cloud)" and "device-first, using the cloud API as a fallback." The logic for *when* to fall back is not defined. | **High** | This is a critical ambiguity in a core feature. What specific conditions trigger the fallback to the cloud API? Is it a specific error from the Health Connect SDK? Is it user-configurable? This logic must be clearly defined. |
| **CCA-004** | **`DataProvider` SDK Packaging Undecided:** `07-apis-integration.md` mentions that for the MVP, a "simpler approach" of keeping the SDK in the main repo could be used instead of a formal private package. It's unclear which path is being taken. | **Medium** | This is a key architectural decision that impacts developer workflow and dependency management. A definitive choice must be made and documented. The formal package is the more scalable approach. |
| **CCA-005** | **Missing API Endpoint for Historical Sync:** As noted in LFI-011, the API contract does not include a way to initiate a historical sync. This is a major gap. | **High** | The API contract needs to be updated. A decision must be made whether to add a `mode` field to the existing `/v1/sync-jobs` endpoint or create a new, dedicated endpoint like `/v1/historical-sync-jobs`. |
| **CCA-006** | **Missing API for User Settings:** The documents describe various user-configurable settings (e.g., `conflictResolutionStrategy`), but there are no API contracts defined for how the user sets these (e.g., `PUT /v1/users/me/settings`). | **High** | The API contract is incomplete. Endpoints for managing user-level settings and sync-level configurations are required. |
| **CCA-007** | **Unclear "Hot User" Identification:** `06-technical-architecture.md` describes a manual process for migrating a "hot user" but doesn't define the process for *identifying* one. How is a hot partition detected? What are the specific metrics and thresholds? | **High** | The process is incomplete. A detailed monitoring and alerting strategy for detecting sustained DynamoDB partition throttling for a specific `USER#{userId}` key is required. This should include specific CloudWatch alarms. |
| **CCA-008** | **Ambiguous `HistoricalOrchestrator` Invocation:** The C4 diagram in `06-technical-architecture.md` shows API Gateway invoking the `HistoricalOrchestrator` (Step Functions). The text confirms this. However, the details of this direct integration are missing. How are parameters passed? How are errors mapped? | **Medium** | The documentation for the API Gateway to Step Functions integration needs to be more detailed, including the request/response mapping templates. |
| **CCA-009** | **Missing Definition for `DateRange`:** The `DataProvider` interface uses a `DateRange` type, but this type is not defined in any of the documents. | **Low** | This is a minor omission but shows a lack of detail. The `DateRange` data class should be defined. |
| **CCA-010** | **Unclear `AI-Powered Merge` Fallback UX:** `05-data-sync.md` states that if the AI merge fails, it falls back to `Prioritize Source` and the UI will show an icon. It's not clear *how* the UI knows this. | **Medium** | The sync result data model needs to be extended. The API that the client calls to get sync history must include a `resolutionMethod` field (e.g., `AI_MERGE`, `FALLBACK_RULE`). This is a missing detail in the API contract. |
| **CCA-011** | **Incomplete "Technology Radar":** The radar in `06-technical-architecture.md` is a good idea but is incomplete. For example, it lists "Unleash" for feature flagging, but the main text says **AWS AppConfig** will be used. | **Medium** | The radar needs to be reconciled with the main body of the document to be useful. The conflict between Unleash and AppConfig must be resolved. |
| **CCA-012** | **Missing Definition for `CanonicalData`:** The `DataProvider` interface uses a `List<CanonicalData>` for the `pushData` method, but the `CanonicalData` sealed interface itself is defined with no implementing classes shown other than `CanonicalWorkout`. What about sleep, steps, etc.? | **Medium** | The documentation of the canonical data models is incomplete. All canonical types (`CanonicalSteps`, `CanonicalSleep`, etc.) that will be used in the MVP must be defined. |
| **CCA-013** | **No Detail on CI/CD for KMP:** The documents mention using KMP for shared logic between mobile and backend. This has significant CI/CD complexity. There are no details on how the KMP module is built, tested, and published for consumption by both the mobile and backend build pipelines. | **High** | This is a major implementation gap. The CI/CD section needs to be expanded to include a detailed workflow for the shared KMP module. |
| **CCA-014** | **Unclear Certificate Pinning Rotation Plan:** `19-security-privacy.md` mentions that certificate pinning is deferred, but that if implemented, the risk would be managed by a "careful certificate rotation plan." This plan is not defined. | **Medium** | While deferred, the complexity of this plan is a key reason *for* deferring it. The document should briefly outline what such a plan would entail (e.g., client-side logic to fetch new pins, forced app updates) to justify the decision to defer. |
| **CCA-015** | **Ambiguous `AnonymizerProxy` Logic:** `06-technical-architecture.md` describes an `AnonymizerProxy` for the AI service. It's not clear what fields are stripped. It says "direct identifiers... are completely removed" and "indirect identifiers... are generalized." This is too vague. | **High** | The anonymization logic must be explicitly defined. A table showing each field in the `CanonicalWorkout` model and the specific anonymization action (remove, hash, generalize) is required for a proper security review. |
| **CCA-016** | **Missing Graceful Degradation Details:** `07-apis-integration.md` mentions using feature flags to put a failing integration into "Read-Only Mode" or "Full Disable". The client-side logic for how the app respects these flags is not described. | **Medium** | How does the client get the feature flag state? Does it fetch them on startup? What is the UI behavior when an integration is disabled? This client-side implementation is missing. |
| **CCA-017** | **Unclear Manual Account Recovery:** `18-backup-recovery.md` describes a manual account recovery process. It's not clear what "updating the Partition Key on all of the user's items" means in practice. This is a very dangerous operation. | **High** | This process needs a much more detailed, step-by-step runbook. It should involve reading all items, writing them with a new PK, and then deleting the old items. A simple "update" is not possible on a partition key. |
| **CCA-018** | **Missing Detail on `DLQAnalyzer`:** `17-error-handling.md` describes a `DLQAnalyzer` Lambda. The logic is high-level ("Known Transient Third-Party Errors"). How are these "known" errors defined and configured? | **Medium** | The configuration mechanism for the analyzer is missing. Will it be a config file in S3 that the Lambda reads on startup? This needs to be specified. |
| **CCA-019** | **No API Contract for Data Export/Deletion:** `19-security-privacy.md` describes the workflows for data export and account deletion, but the specific API endpoints (`POST /v1/export-jobs`, `DELETE /v1/user/me`) are not fully defined in the API contracts section. | **High** | These are legally required features and their API contracts must be formally documented alongside the other core APIs. |
| **CCA-020** | **Vague "Dynamic Chunking Algorithm":** `05-data-sync.md` describes a "dynamic chunking algorithm" for historical syncs that is based on record count. This is a good idea, but it's not clear how the source `DataProvider` can provide a "count of records" for a date range efficiently. Most APIs don't provide a counting endpoint. | **Medium** | This assumption needs to be validated. Does each `DataProvider` support an efficient way to get a record count? If not, the algorithm may need to be changed to a simpler time-based chunking (e.g., 30-day chunks). |
| **CCA-021** | **Missing Client-Side DB Schema:** `06-technical-architecture.md` states SQLDelight will be used for client-side persistence. The schema for this local database is not defined anywhere. | **Medium** | The schema for the local cache (tables for connections, sync settings, offline action queue) needs to be defined to understand the client-side data model. |
| **CCA-022** | **Ambiguous "Source Priority" Conflict Resolution:** `05-data-sync.md` defines the default conflict strategy as "Prioritize Source". It's not clear if this means it overwrites a single conflicting entry or deletes all data in the destination for the overlapping time period before writing the new data. | **Medium** | The exact behavior of this core conflict resolution strategy needs to be specified. "Overwrite" is ambiguous. |
| **CCA-023** | **No Detail on `ProviderManager` Registry:** `06-technical-architecture.md` shows a `ProviderManager` with a "Registry". How is this registry populated? Is it hard-coded? Is it dynamic based on reflection? | **Low** | This is an implementation detail, but it affects how new providers are added. This should be clarified. A simple hard-coded map or a dependency injection framework are likely candidates. |
| **CCA-024** | **Unclear Test Data Management Strategy:** The documents mention extensive testing (unit, integration, E2E). There is no mention of a strategy for managing test data. How are test accounts created? How is the state of backend data reset between test runs? | **High** | A coherent test data management strategy is essential for reliable automated testing. This is a major gap in the "Developer Experience" section. |
| **CCA-025** | **Vague "Governing NFR":** `06-technical-architecture.md` states the "governing NFR is for the system to handle a peak load of 3,000 requests per second (RPS)". This seems to have been chosen arbitrarily. The bottom-up calculation resulted in ~3,125 RPS. | **Low** | The document should clarify why 3,000 was chosen instead of rounding up from the calculated 3,125. Or, simply state the NFR is 3,200 RPS to provide a clear buffer. |

---

## 4. Technical Feasibility Assessment

| ID | Description | Impact/Severity | Recommendation/Question |
|:---|:---|:---|:---|
| **TFA-001** | **Financial Viability of 45,000 Concurrent Lambdas:** The projection of 45,000 concurrent, provisioned JVM-based Lambda executions (`16-performance-optimization.md`) is a massive financial and technical risk. The cost could easily run into hundreds of thousands of dollars per month, which may make the entire business model unsustainable. | **Critical** | The "Mandatory Feasibility Actions" are correct but should be treated as a project-blocking prerequisite. A detailed cost model must be created and approved *before* proceeding. The architecture may need to be fundamentally changed (e.g., to batch-processing in Fargate) to be financially viable. |
| **TFA-002** | **Downstream API Rate Limits:** The architecture assumes that downstream third-party APIs can handle the load generated by up to 45,000 concurrent workers. It's highly unlikely that partners like Fitbit or Strava will tolerate this level of parallelism from a single client. | **Critical** | The distributed rate limiter is a good start, but it's reactive. A proactive strategy is needed. The architecture must include a mechanism to limit the *global* concurrency of workers calling a specific provider's API, not just the rate of calls. This could be a semaphore pattern implemented in Redis. |
| **TFA-003** | **Cross-Cloud Latency for Notifications:** The notification architecture (`29-notifications-alerts.md`) involves a cross-cloud hop from AWS SNS to a Firebase Cloud Function. This introduces latency and a point of failure. | **Medium** | Has the latency of this cross-cloud hop been measured? While the decoupled design is good for security, it may not meet the SLO for time-sensitive alerts if the latency is high. This should be prototyped and measured. |
| **TFA-004** | **Operational Burden of Manual Procedures:** The "break-glass" and "hot user" procedures are defined as manual, PR-based workflows. At 1M DAU, the frequency of these events will be non-trivial. Relying on manual engineering intervention is not operationally feasible and will lead to slow support responses and potential cascading failures. | **High** | These manual processes are a major feasibility risk. The roadmap must include the development of secure, audited internal tools to automate these workflows *before* the user base grows significantly. Deferring this is a major risk. |
| **TFA-005** | **KMP on Lambda Cold Starts:** The use of a KMP/JVM runtime for the `WorkerLambda` (`06-technical-architecture.md`) is a performance risk. Even with provisioned concurrency, a sudden spike in traffic beyond the provisioned pool will result in very slow cold starts (potentially 5-10 seconds), which could cause a cascading backlog. | **High** | The decision to use a JVM-based runtime for a massively parallel Lambda workload needs to be carefully benchmarked. A native-compiled KMP target (if available) or a different backend language (Go, Rust, TypeScript) should be seriously considered for the worker fleet to minimize the impact of cold starts. |
| **TFA-006** | **Complexity of `DataProvider` SDK:** The plan to build a versioned, packaged `DataProvider` SDK (`07-apis-integration.md`) is a significant engineering effort in itself. It risks becoming a project-within-a-project and slowing down the initial delivery of integrations. | **Medium** | For an MVP, the "simpler approach" of keeping the provider logic in a monorepo is likely more feasible. The formal SDK can be extracted later. The current plan may be over-engineered for the initial launch. |
| **TFA-007** | **VPC Networking Limits:** Running 45,000 concurrent Lambdas in a VPC will place immense pressure on networking resources, such as the number of available IP addresses in the subnets and the throughput of the NAT Gateway (for traffic to non-AWS services). | **High** | The resource planning must include a detailed analysis of VPC IP address allocation and NAT Gateway scaling limits. This is a common and often overlooked failure point for large-scale serverless applications. |
| **TFA-008** | **Feasibility of "Break-Glass" Index:** The `SyncWellBreakGlassIndex` (`19-security-privacy.md`) is populated by the `AuthorizerLambda`. This adds a write operation to a critical, latency-sensitive path. This could impact the P99 latency of all API requests. | **Medium** | The performance impact of this additional write operation on the authorizer must be measured. It may be better to have the authorizer publish an event asynchronously, and have a separate Lambda handle the write to the index, decoupling it from the synchronous request path. |
| **TFA-009** | **Realism of Manual Runbooks:** The documents contain several detailed manual runbooks (e.g., for PITR, manual account recovery). While detailed, they assume a high level of engineer proficiency under pressure. A single mistake in a PITR recovery could be catastrophic. | **High** | These critical runbooks must be automated as much as possible via peer-reviewed, version-controlled scripts. They should also be regularly practiced in a staging environment as part of "game day" exercises to ensure they work and that engineers are comfortable executing them. |
| **TFA-010** | **Firebase Auth Dependency:** The choice of Firebase Auth over Cognito (`06-technical-architecture.md`) is well-justified for developer experience. However, it introduces a hard dependency on a different cloud provider for the most critical user-facing function: login. An outage at Google could render the entire SyncWell platform inaccessible. | **High** | This is a major strategic risk that must be formally accepted by the business. A contingency plan, even if it's just a pre-written status page and user communication, should be prepared for a major Firebase Auth outage. |
| **TFA-011** | **Jailbreak/Root Detection Reliability:** `19-security-privacy.md` mentions jailbreak/root detection as a defense-in-depth measure. This is notoriously unreliable and can be easily bypassed. Over-reliance on it can create a false sense of security. | **Low** | The document should be clearer that this is a "best-effort" measure and should not be relied upon as a primary security control. |
| **TFA-012** | **Complexity of Circuit Breaker:** `07-apis-integration.md` proposes a per-provider circuit breaker. Implementing a distributed circuit breaker is complex. | **Medium** | Is a full circuit breaker pattern necessary for the MVP, or would simpler, per-provider error rate alarms be sufficient to start? This adds significant complexity to the `DataProvider` SDK. |
| **TFA-013** | **Scalability of Step Functions:** The use of Step Functions for historical syncs is excellent. However, Step Functions has its own service limits (e.g., execution history size, start rate). These limits should be analyzed against the projected number of historical syncs. | **Medium** | A brief analysis of Step Functions service limits in the context of the projected load should be added to the document to confirm it's a feasible choice. |
| **TFA-014** | **Feasibility of "Hot User" De-migration:** The process for migrating a "hot user" back from the dedicated table is mentioned but not detailed. This is a non-trivial process that involves potential downtime for that user. | **Medium** | The de-migration process needs a detailed runbook, just like the migration process. How are writes handled during the de-migration? |
| **TFA-015** | **LocalStack Fidelity:** The dev-ex section relies heavily on LocalStack. While excellent, LocalStack does not have perfect fidelity with all AWS services, especially for complex IAM policies or new service features. | **Medium** | The team should be aware of LocalStack's limitations and have a strategy for integration testing in a real AWS "dev" environment as part of the CI/CD pipeline, not just relying on local tests. |

---

## 5. Risk and Gap Analysis

### Security Risks
| ID | Description | Impact/Severity | Recommendation/Question |
|:---|:---|:---|:---|
| **SEC-001** | **Lack of Egress Controls in Design:** While `19-security-privacy.md` mentions an AWS Network Firewall for egress traffic, this is not reflected in any of the architectural diagrams. The diagrams imply Lambda functions have direct internet access. | **Critical** | This is a major security gap. An egress firewall is a critical defense-in-depth measure. The architecture diagrams must be updated to show the VPC, subnets, and the Network Firewall controlling all outbound traffic from the worker Lambdas. |
| **SEC-002** | **No Rate Limiting on Auth Endpoints:** The API contracts do not specify any rate limiting on security-sensitive endpoints like the one that exchanges an auth code for tokens. This endpoint could be abused by an attacker to probe for valid codes. | **High** | All public-facing API endpoints, especially those involved in authentication, must have rate limiting configured (e.g., via AWS WAF). |
| **SEC-003** | **Potential for Leaking `correlationId` -> `userId`:** The break-glass procedure is a necessary evil, but the existence of the `SyncWellBreakGlassIndex` creates a high-value target for an attacker. A compromise of this table would de-anonymize all logs. | **High** | Access to this table needs to be the most stringently controlled resource in the entire system. The document should specify that this table should have a different, more restrictive IAM access policy than any other data store. Consider encrypting it with a customer-managed KMS key. |
| **SEC-004** | **Unclear Security of "Manual Recovery" Script:** The manual account recovery process involves a "peer-reviewed, version-controlled script." Where is this script stored? Who has access to run it? A vulnerability in this script could allow an engineer to perform unauthorized account takeovers. | **High** | The security and access control for this critical operational script needs to be defined. It should be stored in a private repo with branch protection rules, and only a very small, named group of engineers should have permission to execute it. |
| **SEC-005** | **No Security Review for AI Service:** The document states the AI service will undergo a "rigorous security and privacy review" in the future. This is a risk. Security cannot be an afterthought for a component that processes user data, even if anonymized. | **High** | The high-level security design for the AI service must be defined *now*, not later. This includes data flow, threat modeling, and input sanitization strategies. |
| **SEC-006** | **No Mention of AWS WAF:** The documents do not mention using AWS WAF (Web Application Firewall) in front of API Gateway. This is a standard best practice for protecting against common web exploits like SQL injection, cross-site scripting, and bot traffic. | **High** | AWS WAF should be added to the architecture in front of API Gateway as a foundational security layer. |
| **SEC-007** | **Secret Rotation Not Mentioned:** AWS Secrets Manager supports automatic rotation of secrets. The documentation does not mention if this feature will be used for the OAuth `client_secret` values for SyncWell's own applications. | **Medium** | A strategy for rotating the application-level `client_secret`s for each third-party provider should be defined. Using Secrets Manager's automated rotation feature is a best practice. |
| **SEC-008** | **Lack of DAST in CI/CD:** The CI/CD pipeline in `06-technical-architecture.md` mentions SAST (Static Analysis) but not DAST (Dynamic Analysis). | **Medium** | For a comprehensive security posture, DAST should be integrated into the pipeline to scan the running application in the staging environment for vulnerabilities. |
| **SEC-009** | **Insecure S3 Bucket for Data Export:** `19-security-privacy.md` mentions saving data exports to an S3 bucket. Are these buckets configured to block public access? Are the objects encrypted at rest? | **High** | The security configuration for the S3 bucket used for data exports must be explicitly defined, including "Block Public Access" settings, default encryption, and restrictive lifecycle policies to delete old exports. |
| **SEC-010** | **No Mention of Security Headers:** There is no mention of standard security headers (like `Strict-Transport-Security`, `Content-Security-Policy`, `X-Content-Type-Options`) being returned by the API Gateway. | **Medium** | These headers are a simple and effective defense-in-depth measure. They should be configured on the API Gateway responses. |

### Operational Risks
| ID | Description | Impact/Severity | Recommendation/Question |
|:---|:---|:---|:---|
| **OPS-001** | **Lack of Centralized Feature Flag Management:** The architecture mentions both AWS AppConfig and Unleash for feature flagging. Using two systems creates operational confusion. | **High** | A single, definitive tool for feature flagging and remote configuration must be chosen and used consistently. AWS AppConfig is the more integrated choice given the rest of the stack. |
| **OPS-002** | **No On-Call Rotation or Escalation Policy Defined:** The documents frequently mention "alerting the on-call engineer" but do not define the on-call process itself. Who is on-call? What is the rotation schedule? What is the escalation path if an alert is not acknowledged? | **High** | This is a major operational gap. A formal on-call policy, schedule, and escalation path (e.g., primary -> secondary -> engineering manager) must be defined and managed in a tool like PagerDuty. |
| **OPS-003** | **No Post-Mortem Process Defined:** The PITR runbook mentions a post-mortem, but there is no formal process defined for this. | **High** | A formal, blameless post-mortem process is critical for learning from incidents. A template and procedure for conducting post-mortems should be created. |
| **OPS-004** | **Vague "Game Day" Testing:** The recommendation to practice runbooks is good, but there's no commitment or schedule for it. | **Medium** | The SRE/Maintenance plan should include a formal schedule for "game day" exercises (e.g., quarterly) where the team practices failure scenarios in a staging environment. |
| **OPS-005** | **No Cost Monitoring Strategy:** The documents acknowledge that costs will be high but do not specify a strategy for *monitoring* costs. | **High** | A cost monitoring strategy is essential. This should include setting up AWS Cost Anomaly Detection alerts and creating a monthly budget vs. actuals review process. |
| **OPS-006** | **Lack of Centralized Runbook Repository:** The documents contain snippets of runbooks, but there is no mention of a centralized, version-controlled repository for all operational runbooks. | **Medium** | All runbooks should be stored as Markdown files in a dedicated Git repository, so they are version-controlled and can be improved via a PR process. |
| **OPS-007** | **No Plan for Database Schema Migrations:** The DynamoDB single-table design is well-defined, but what happens when it needs to change? There is no mention of a strategy for performing schema migrations on the DynamoDB table. | **High** | A strategy for handling data migrations is a critical operational gap. Will it be a script-based approach? Will a tool like the DynamoDB Migrator be used? This needs to be defined. |

### "Unknown Unknowns" & Alternative Approaches
| ID | Description | Impact/Severity | Recommendation/Question |
|:---|:---|:---|:---|
| **ALT-001** | **Alternative to Pure Lambda for Workers:** The documents briefly mention Fargate as a future alternative. Has a batch-processing model within a single Lambda been considered? Instead of one Lambda per job, one Lambda could pull 10 messages from SQS and process them sequentially or in parallel internally. | **High** | This is a key architectural alternative that could drastically reduce the concurrency requirement (from 45,000 to 4,500) and the associated cost. This alternative should be assessed *before* committing to the 1-job-per-Lambda model. |
| **ALT-002** | **Alternative to Cross-Cloud Auth:** Was a "backend-for-frontend" (BFF) pattern considered for authentication? The mobile app could talk to a lightweight BFF in the AWS ecosystem, which then brokers the authentication with Firebase. This would contain the cross-cloud dependency to a single service. | **Medium** | This alternative could offer a better balance of concerns, leveraging the Firebase SDKs on the client while keeping the primary backend infrastructure single-cloud. Was this pattern evaluated? |
| **ALT-003** | **What if a User is a "Hot Write" User?** The "hot user" strategy focuses on read-heavy users. What if a user generates an enormous volume of *writes*? A dedicated table might still be throttled. Was write-sharding (splitting a user's data across multiple partitions like `USER#{userId}-1`, `USER#{userId}-2`) considered? | **High** | The document should acknowledge the "hot write" scenario as a separate and more complex problem, and briefly mention write-sharding as the potential, albeit more complex, long-term solution. |
| **ALT-004** | **Using Step Functions for the "Hot Path":** The document separates hot/cold paths between SQS/Lambda and Step Functions. Why not use Step Functions Express Workflows for the hot path? They are designed for high-volume, short-duration workflows and provide better observability than a simple SQS->Lambda trigger. | **Medium** | This is a viable architectural alternative that could unify the orchestration model. The pros and cons of using Express Workflows for the hot path should be evaluated. |
| **ALT-005** | **Alternative to "Break-Glass" Index:** Instead of a purpose-built DynamoDB table for the break-glass index, could the system temporarily enable logging of the `userId` for a specific user via a feature flag in AppConfig, and then automatically disable it after a short period? | **Medium** | This alternative approach might be simpler and avoid creating a permanent high-value target data store. It's worth considering if it's technically feasible with the logging framework. |
| **ALT-006** | **GraphQL as an API Alternative:** The API is defined as a standard REST/JSON API. Was GraphQL considered? For a client-heavy application with complex state, GraphQL could reduce the number of round trips and allow the client to fetch only the data it needs. | **Low** | This is a major architectural choice, but the document doesn't show any evidence that it was considered. It's worth a brief mention in the "Hold" section of the technology radar. |
